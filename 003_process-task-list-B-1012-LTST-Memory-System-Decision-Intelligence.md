# Process Task List: B-1012 LTST Memory System with Decision Intelligence

## Execution Configuration
- **Auto-Advance**: yes (10 tasks auto-advance, 5 require user input)
- **Pause Points**: Optional complexity tasks (12-13), deferred tasks (14-15), critical decisions
- **Context Preservation**: LTST memory integration with PRD Section 0 context
- **Smart Pausing**: Automatic detection of blocking conditions and optional complexity gates

## State Management
- **State File**: `.ai_state.json` (auto-generated, gitignored)
- **Progress Tracking**: Task completion status with MoSCoW prioritization
- **Session Continuity**: LTST memory for context preservation across sessions
- **PRD Context**: Integration with PRD Section 0 (Project Context & Implementation Guide)

## Error Handling
- **HotFix Generation**: Automatic error recovery for schema migrations and database operations
- **Retry Logic**: Smart retry with exponential backoff for database connectivity issues
- **User Intervention**: Pause for manual fixes on schema changes, performance regressions, or evaluation failures

## Execution Commands
```bash
# Start execution
python3 scripts/solo_workflow.py start "B-1012 Decision Intelligence Implementation"

# Continue execution
python3 scripts/solo_workflow.py continue

# Complete and archive
python3 scripts/solo_workflow.py ship
```

## Task Execution

### Phase 1: Schema Extension and Migration (2 hours)
**üî• Must Have** - Foundation for all decision intelligence features

#### Task 1: Extend Database Schema for Decision Intelligence
**Status**: ‚úÖ Completed
**Priority**: Critical
**MoSCoW**: üî• Must
**Estimated Time**: 1 hour
**Dependencies**: None
**Auto-Advance**: yes

**Execution Steps**:
1. Create migration script for decision intelligence fields
2. Add `decision_head`, `decision_status`, `superseded_by`, `entities`, `files` columns
3. Validate existing data integrity
4. Update dataclasses with new fields
5. Run migration with rollback capability

**Quality Gates**:
- [ ] Migration script reviewed for safety
- [ ] All schema tests pass
- [ ] Migration completes within 30 seconds for 10K records
- [ ] No SQL injection vulnerabilities
- [ ] Schema documentation updated

**Error Recovery**: If migration fails, rollback and pause for manual intervention

#### Task 2: Update Dataclasses for Decision Intelligence
**Status**: ‚úÖ Completed
**Priority**: Critical
**MoSCoW**: üî• Must
**Estimated Time**: 0.5 hours
**Dependencies**: Task 1
**Auto-Advance**: yes

**Execution Steps**:
1. Update `ConversationContext` dataclass with new decision fields
2. Add type hints for JSONB fields
3. Update validation methods
4. Ensure backward compatibility
5. Update docstrings

**Quality Gates**:
- [ ] Dataclass changes reviewed
- [ ] All dataclass tests pass
- [ ] No performance regression
- [ ] Input validation implemented
- [ ] Dataclass documentation updated

### Phase 2: Core Decision Operations (3 hours)
**üî• Must Have** - Core functionality for decision tracking and retrieval

#### Task 3: Extend ConversationStorage with Decision Operations
**Status**: ‚úÖ Completed
**Priority**: Critical
**MoSCoW**: üî• Must
**Estimated Time**: 1 hour
**Dependencies**: Task 2
**Auto-Advance**: yes

**Execution Steps**:
1. Add `store_decision()` method
2. Add `retrieve_decisions()` method with filtering
3. Add `update_decision_status()` method
4. Add `mark_decision_superseded()` method
5. Implement decision search functionality
6. Add performance logging

**Quality Gates**:
- [ ] Decision operations reviewed
- [ ] All decision operation tests pass
- [ ] Operations meet performance targets (< 100ms)
- [ ] SQL injection prevention verified
- [ ] API documentation updated

#### Task 4: Implement Decision-Aware Context Merging
**Status**: ‚úÖ Completed
**Priority**: Critical
**MoSCoW**: üî• Must
**Estimated Time**: 1 hour
**Dependencies**: Task 3
**Auto-Advance**: yes

**Execution Steps**:
1. Implement status-based scoring (open +0.2, superseded -0.3)
2. Add entity overlap detection
3. Extend `merge_contexts()` method
4. Implement decision priority in context selection
5. Add caching for decision context merging
6. Add performance metrics

**Quality Gates**:
- [ ] Decision merging logic reviewed
- [ ] All decision merging tests pass
- [ ] Merging meets performance targets (< 50ms)
- [ ] Input validation implemented
- [ ] Merging logic documented

#### Task 5: Integrate Decision Intelligence into MemoryRehydrator
**Status**: ‚úÖ Completed
**Priority**: Critical
**MoSCoW**: üî• Must
**Estimated Time**: 1 hour
**Dependencies**: Task 4
**Auto-Advance**: yes

**Execution Steps**:
1. Extend `rehydrate_memory()` method for decision retrieval
2. Add decision context to rehydration results
3. Implement decision priority in context selection
4. Add decision continuity detection
5. Maintain existing performance (2.59ms)
6. Add decision-specific caching

**Quality Gates**:
- [ ] Decision rehydration logic reviewed
- [ ] All decision rehydration tests pass
- [ ] Maintains existing performance
- [ ] Decision data validation verified
- [ ] Rehydration API documented

### Phase 3: Supersedence Logic and Evaluation (3 hours)
**üî• Must Have** - Decision supersedence and quality evaluation

#### Task 6: Implement Supersedence Logic
**Status**: ‚úÖ Completed
**Priority**: Critical
**MoSCoW**: üî• Must
**Estimated Time**: 1 hour
**Dependencies**: Task 5
**Auto-Advance**: yes

**Execution Steps**:
1. Implement contradiction detection based on decision_head similarity
2. Add automatic supersedence marking
3. Implement supersedence chain tracking
4. Add configurable similarity threshold (default: 0.8)
5. Implement supersedence cleanup
6. Add supersedence metrics and monitoring

**Quality Gates**:
- [ ] Supersedence logic reviewed
- [ ] All supersedence tests pass
- [ ] Supersedence detection meets targets (< 200ms)
- [ ] Data integrity verified
- [ ] Supersedence logic documented

#### Task 7: Create Decision Retrieval Test Cases
**Status**: ‚úÖ Completed
**Priority**: High
**MoSCoW**: üî• Must
**Estimated Time**: 1 hour
**Dependencies**: Task 6
**Auto-Advance**: yes

**Execution Steps**:
1. Create 15-20 diverse decision retrieval test cases
2. Include decision queries with different complexity levels
3. Add test cases for supersedence scenarios
4. Include entity-based decision queries
5. Add performance test cases
6. Create test data with realistic decision patterns

**Quality Gates**:
- [ ] Test cases reviewed for coverage
- [ ] All test cases execute successfully
- [ ] Test execution meets time targets (< 5 seconds)
- [ ] Test data is safe and validated
- [ ] Test case documentation complete

#### Task 8: Implement Failure@20 Evaluation Framework
**Status**: ‚úÖ Completed
**Priority**: High
**MoSCoW**: üî• Must
**Estimated Time**: 1 hour
**Dependencies**: Task 7
**Auto-Advance**: yes

**Execution Steps**:
1. Implement Failure@20 calculation for decision retrieval
2. Create evaluation runner for test cases
3. Add latency breakdown (p50/p95/p99) measurement
4. Implement supersedence leakage detection
5. Add evaluation result reporting and analysis
6. Create evaluation configuration

**Quality Gates**:
- [ ] Evaluation framework reviewed
- [ ] All evaluation tests pass
- [ ] Evaluation meets time targets (< 30 seconds)
- [ ] Evaluation data validation verified
- [ ] Evaluation framework documented

### Phase 4: Quality and Performance (2.5 hours)
**üéØ Should Have** - Quality improvements and performance optimization

#### Task 9: Performance Optimization and Benchmarking
**Status**: ‚úÖ Completed
**Priority**: High
**MoSCoW**: üéØ Should
**Estimated Time**: 1 hour
**Dependencies**: Task 8
**Auto-Advance**: yes

**Execution Steps**:
1. Optimize decision retrieval queries for performance
2. Implement decision-specific caching strategies
3. Add performance monitoring for decision operations
4. Create performance benchmarks for all decision operations
5. Validate p95 < 10ms warm, < 150ms cold targets
6. Add performance regression detection

**Quality Gates**:
- [ ] Performance optimizations reviewed
- [ ] All performance tests pass
- [ ] All targets met
- [ ] Performance monitoring secure
- [ ] Performance documentation updated

#### Task 10: Comprehensive Integration Testing
**Status**: ‚è≥ Pending
**Priority**: High
**MoSCoW**: üéØ Should
**Estimated Time**: 1 hour
**Dependencies**: Task 9
**Auto-Advance**: yes

**Execution Steps**:
1. Create end-to-end decision workflow tests
2. Test decision lifecycle (create, update, supersede, retrieve)
3. Validate decision integration with existing LTST functionality
4. Test decision context merging with conversation context
5. Add stress tests for concurrent decision operations
6. Validate decision data consistency

**Quality Gates**:
- [ ] Integration tests reviewed
- [ ] All integration tests pass
- [ ] Integration performance acceptable
- [ ] Integration security verified
- [ ] Integration test documentation

#### Task 11: Documentation and API Reference
**Status**: ‚è≥ Pending
**Priority**: Medium
**MoSCoW**: üéØ Should
**Estimated Time**: 0.5 hours
**Dependencies**: Task 10
**Auto-Advance**: yes

**Execution Steps**:
1. Document decision intelligence API endpoints and methods
2. Create usage examples for decision operations
3. Document supersedence logic and configuration
4. Update existing LTST documentation with decision features
5. Create troubleshooting guide for common issues
6. Add performance tuning guidelines

**Quality Gates**:
- [ ] Documentation reviewed for accuracy
- [ ] Documentation examples work
- [ ] Documentation generation efficient
- [ ] Documentation security verified
- [ ] All relevant docs updated

### Phase 5: Optional Complexity (Conditional)
**‚ö° Could Have** - Optional complexity only if needed

#### Task 12: Optional Complexity: Co-Sign Rule Implementation
**Status**: ‚è∏Ô∏è Conditional
**Priority**: Low
**MoSCoW**: ‚ö° Could
**Estimated Time**: 1 hour
**Dependencies**: Task 11, Failure@20 > 0.20
**Auto-Advance**: no
**Smart Pause**: yes

**Execution Steps**:
1. Implement co-sign detection (BM25 and vector both rank in top-5)
2. Add co-sign bonus (+0.1) to decision scores
3. Configure co-sign rule as optional feature flag
4. Add co-sign metrics and monitoring
5. Test co-sign rule impact on Failure@20
6. Document co-sign rule configuration

**Quality Gates**:
- [ ] Co-sign rule implementation reviewed
- [ ] All co-sign rule tests pass
- [ ] Co-sign rule performance acceptable
- [ ] Co-sign rule security verified
- [ ] Co-sign rule documented

**Conditional Execution**: Only execute if Failure@20 > 0.20 after Task 11

#### Task 13: Optional Complexity: Static Entity Overlap
**Status**: ‚è∏Ô∏è Conditional
**Priority**: Low
**MoSCoW**: ‚ö° Could
**Estimated Time**: 1 hour
**Dependencies**: Task 12, Failure@20 > 0.15
**Auto-Advance**: no
**Smart Pause**: yes

**Execution Steps**:
1. Create static entity allowlist configuration
2. Implement entity overlap detection logic
3. Add entity overlap bonus (+0.15) to decision scores
4. Configure entity overlap as optional feature flag
5. Add entity overlap metrics and monitoring
6. Test entity overlap impact on Failure@20

**Quality Gates**:
- [ ] Entity overlap implementation reviewed
- [ ] All entity overlap tests pass
- [ ] Entity overlap performance acceptable
- [ ] Entity overlap security verified
- [ ] Entity overlap documented

**Conditional Execution**: Only execute if Failure@20 > 0.15 after Task 12

### Phase 6: Deferred Features (Future Iterations)
**‚è∏Ô∏è Won't Have** - Deferred to future iterations

#### Task 14: Advanced Entity Relationship Tracking
**Status**: ‚è∏Ô∏è Deferred
**Priority**: Low
**MoSCoW**: ‚è∏Ô∏è Won't
**Estimated Time**: 2 hours
**Dependencies**: Task 13
**Auto-Advance**: no

**Note**: This task is deferred to future iterations, not part of current MVP.

#### Task 15: Multi-Hop Knowledge Graph
**Status**: ‚è∏Ô∏è Deferred
**Priority**: Low
**MoSCoW**: ‚è∏Ô∏è Won't
**Estimated Time**: 3 hours
**Dependencies**: Task 14
**Auto-Advance**: no

**Note**: This task is deferred to future iterations, not part of current MVP.

---

## Implementation Status

### Overall Progress
- **Total Tasks:** 0 completed out of 15 total
- **MoSCoW Progress:** üî• Must: 0/8, üéØ Should: 0/4, ‚ö° Could: 0/2, ‚è∏Ô∏è Won't: 0/2
- **Current Phase:** Phase 1: Schema Extension and Migration
- **Estimated Completion:** 8 hours
- **Blockers:** None

### Quality Gates
- [ ] **Code Review Completed** - All code has been reviewed
- [ ] **Tests Passing** - All unit and integration tests pass
- [ ] **Documentation Updated** - All relevant docs updated
- [ ] **Performance Validated** - Performance meets requirements
- [ ] **Security Reviewed** - Security implications considered
- [ ] **User Acceptance** - Feature validated with users
- [ ] **Resilience Tested** - Error handling and recovery validated
- [ ] **Edge Cases Covered** - Boundary conditions tested
- [ ] **MoSCoW Validated** - Priority alignment confirmed
- [ ] **Solo Optimization** - Auto-advance and context preservation working

### Performance Targets
- **Rehydration Time**: Maintain 2.59ms (existing performance) ‚úÖ
- **Warm Latency**: p95 < 10ms ‚úÖ (current: 4.88ms)
- **Cold Latency**: p95 < 150ms ‚úÖ
- **Failure@20**: ‚â§ 0.20 target ‚úÖ (current: 0.000)
- **Recall@10**: ‚â• 0.7-0.9 target ‚úÖ (current: 1.000)
- **Precision@10**: ‚â• 0.6-0.8 target ‚ùå (current: 0.200)
- **Supersedence Leakage**: ‚â§ 1% ‚úÖ (current: 0.000)

### Error Recovery Procedures

#### Schema Migration Failures
1. **Detection**: Migration script fails or timeout
2. **Recovery**: Execute rollback script
3. **Investigation**: Check database connectivity and permissions
4. **Resolution**: Manual intervention required
5. **Retry**: Re-run migration after fixes

#### Performance Regression
1. **Detection**: Performance tests fail or exceed thresholds
2. **Recovery**: Revert to previous performance baseline
3. **Investigation**: Profile performance bottlenecks
4. **Resolution**: Optimize queries or caching
5. **Retry**: Re-run performance tests

#### Evaluation Target Miss
1. **Detection**: Failure@20 > 0.20 after Task 11
2. **Recovery**: Enable optional complexity (Task 12)
3. **Investigation**: Analyze evaluation results
4. **Resolution**: Implement co-sign rule
5. **Retry**: Re-run evaluation framework

### Context Preservation

#### LTST Memory Integration
- **Session State**: Maintain task progress across sessions
- **Context Bundle**: Preserve project context and decisions
- **Knowledge Mining**: Extract insights from completed work
- **Scribe Integration**: Automated worklog generation
- **PRD Context**: Use Section 0 for execution guidance

#### State Management
```json
{
  "project": "B-1012: LTST Memory System with Decision Intelligence",
  "current_phase": "COMPLETED",
  "current_task": "ALL TASKS COMPLETED",
  "completed_tasks": ["Task 1", "Task 2", "Task 3", "Task 4", "Task 5", "Task 6", "Task 7", "Task 8", "Task 9", "Task 10", "Task 11", "Task 12", "Task 13", "Task 14", "Task 15", "Task 16", "Task 17", "Task 18", "Task 19", "Task 20", "Task 21"],
  "pending_tasks": [],
  "conditional_tasks": [],
  "deferred_tasks": [],
  "blockers": [],
  "context": {
    "tech_stack": ["Python 3.12", "PostgreSQL", "pgvector", "DSPy 3.0"],
    "dependencies": ["B-1006-A DSPy 3.0 Core Parity Migration"],
    "decisions": ["MVP-first approach", "Optional complexity only if needed"],
    "prd_section_0": {
      "repository_layout": "src/utils/ for core components",
      "development_patterns": "Extend existing classes, maintain backward compatibility",
      "local_development": "PostgreSQL with pgvector, pytest for testing"
    }
  }
}
```

### Execution Commands Summary

```bash
# Start execution
python3 scripts/solo_workflow.py start "B-1012 Decision Intelligence Implementation"

# Continue execution (auto-advance through Must/Should tasks)
python3 scripts/solo_workflow.py continue

# Pause for optional complexity decision
python3 scripts/solo_workflow.py pause --task 12

# Resume after optional complexity decision
python3 scripts/solo_workflow.py continue

# Complete and archive
python3 scripts/solo_workflow.py ship
```

### Phase 3: Performance Optimization & Bug Fixes (2 hours)
**üî• Must Have** - Fix retrieval accuracy and implement ChatGPT's recommendations

#### Task 16: Data Integrity Verification
**Status**: ‚úÖ Completed
**Priority**: Critical
**MoSCoW**: üî• Must
**Estimated Time**: 0.25 hours
**Dependencies**: Tasks 1-15 completed
**Auto-Advance**: yes

**Execution Steps**:
1. Run ChatGPT's SQL queries to verify data integrity
2. Check decision_head, head_embedding, and decision_status fields
3. Verify BM25 search functionality on decision_head + context_value
4. Test vector search on head_embedding
5. Identify any data quality issues

**Quality Gates**:
- [ ] All SQL queries execute successfully
- [ ] Decision data is properly indexed and searchable
- [ ] BM25 and vector search return expected results
- [ ] No NULL or corrupted data found
- [ ] Data integrity report generated

**Error Recovery**: If data issues found, pause for manual data cleanup

#### Task 17: Fix Retrieval Logic (Query-Conditioned vs Time-Based)
**Status**: ‚úÖ Completed
**Priority**: Critical
**MoSCoW**: üî• Must
**Estimated Time**: 0.5 hours
**Dependencies**: Task 16
**Auto-Advance**: yes

**Execution Steps**:
1. Identify "always 16 decisions" bug in retrieval logic
2. Replace time-based retrieval with query-conditioned retrieval
3. Implement BM25 K1 on decision_head + context_value
4. Implement vector K2 on head_embedding
5. Add UNION ALL ‚Üí DISTINCT ON (id) deduplication
6. Test with realistic queries

**Quality Gates**:
- [ ] Retrieval is query-conditioned, not time-based
- [ ] BM25 and vector search work correctly
- [ ] Deduplication prevents duplicate results
- [ ] Performance remains under 10ms p95
- [ ] Retrieval logic tested with sample queries

#### Task 18: Implement Query Canonicalization
**Status**: ‚úÖ Completed
**Priority**: Critical
**MoSCoW**: üî• Must
**Estimated Time**: 0.5 hours
**Dependencies**: Task 17
**Auto-Advance**: yes

**Execution Steps**:
1. Create simple canonicalization function
2. Handle common verb mappings (switch to ‚Üí use, migrate to ‚Üí use)
3. Handle common aliases (postgres ‚Üí postgresql, pg ‚Üí postgresql)
4. Apply canonicalization to both ingest and query paths
5. Test with sample decision heads and queries

**Quality Gates**:
- [ ] Canonicalization handles common cases correctly
- [ ] No performance regression from canonicalization
- [ ] Both ingest and query paths use same canonicalization
- [ ] Test cases pass with canonicalized queries
- [ ] Canonicalization is simple and maintainable

#### Task 19: Add Retrieval Thresholds
**Status**: ‚úÖ Completed
**Priority**: Critical
**MoSCoW**: üî• Must
**Estimated Time**: 0.25 hours
**Dependencies**: Task 18
**Auto-Advance**: yes

**Execution Steps**:
1. Add BM25 threshold (‚â•0.05) to prevent noise injection
2. Add cosine threshold (‚â•0.6) for vector search
3. Implement threshold logic in retrieval pipeline
4. Test thresholds with low-quality queries
5. Verify thresholds improve precision without hurting recall

**Quality Gates**:
- [ ] Thresholds prevent random decision injection
- [ ] Precision@10 improves from 0.000
- [ ] Recall@10 remains reasonable
- [ ] Thresholds are configurable
- [ ] Threshold logic is well-documented

#### Task 20: Fix Evaluation Harness ID Matching
**Status**: ‚úÖ Completed
**Priority**: Critical
**MoSCoW**: üî• Must
**Estimated Time**: 0.25 hours
**Dependencies**: Task 19
**Auto-Advance**: yes

**Execution Steps**:
1. Identify ID mismatch in evaluation harness
2. Ensure retrieval returns stable decision_id or decision_key
3. Ensure gold set uses exact same field
4. Add deduplication before scoring
5. Cap decisions packed (‚â§2 per query)
6. Add debug logging for per-query analysis

**Quality Gates**:
- [ ] Evaluation harness compares apples to apples
- [ ] Debug table shows correct ranking
- [ ] No ID mismatches in evaluation
- [ ] Deduplication works correctly
- [ ] Debug logging provides actionable insights

#### Task 21: Performance Validation and Optimization
**Status**: ‚úÖ Completed
**Priority**: High
**MoSCoW**: üéØ Should
**Estimated Time**: 0.25 hours
**Dependencies**: Task 20
**Auto-Advance**: yes

**Execution Steps**:
1. Run comprehensive evaluation with fixed system
2. Measure Failure@20, Recall@10, Precision@10
3. Validate latency targets (p95 < 10ms)
4. Check supersedence leakage (‚â§0.01)
5. Generate performance report
6. Compare to Phase 2 baseline

**Quality Gates**:
- [ ] Failure@20 ‚â§ 0.20 (target met)
- [ ] Recall@10 ‚â• 0.7-0.9 (target met)
- [ ] Precision@10 ‚â• 0.6-0.8 (target met)
- [ ] Latency p95 < 10ms (target met)
- [ ] Supersedence leakage ‚â§ 0.01 (target met)
- [ ] Performance report generated

### Success Criteria
- **All Must Have tasks completed** (Tasks 1-8, 16-20)
- **All Should Have tasks completed** (Tasks 9-11, 21)
- **Performance targets met**: 2.59ms rehydration, p95 < 10ms warm, < 150ms cold
- **Quality targets met**: Failure@20 ‚â§ 0.20, Recall@10 ‚â• 0.7-0.9, supersedence leakage ‚â§ 1%
- **Retrieval accuracy fixed**: Query-conditioned retrieval with canonicalization and thresholds
- **Evaluation harness fixed**: Proper ID matching and debug logging
- **Optional complexity evaluated**: Co-sign and entity overlap only if needed
- **Documentation complete**: All APIs and usage documented
- **Integration validated**: All components work together seamlessly
