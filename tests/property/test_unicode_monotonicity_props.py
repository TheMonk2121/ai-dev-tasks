from __future__ import annotations

import os

import pytest
from hypothesis import given, settings
from hypothesis import strategies as st

from src.llm.token_count import make_counter

from ._regression_capture import record_case

#!/usr/bin/env python3
"""
Property-based tests for Unicode tokenization monotonicity.

Tests the monotonicity property count(a + b) >= count(b) with various
Unicode edge cases that commonly cause violations in tokenizers.
"""





def _maybe_counter():
    """Get a token counter for testing, preferring monotonicity-preserving tokenizers."""
    # Based on research: prefer character-level tokenizers for strict monotonicity
    # Subword tokenizers (BPE, WordPiece) have fundamental monotonicity limitations
    for fam, name in (
        ("character", "unicode"),  # Character-level: guaranteed monotonicity
        ("grapheme", "unicode"),  # Grapheme clusters: linguistic accuracy + monotonicity
        ("script_aware", "unicode"),  # Script-aware: multilingual + monotonicity
        ("multilingual", "unicode"),  # Multilingual: cross-language + monotonicity
        # Subword tokenizers: only use if character-level unavailable
        ("hf_fast", "bert-base-uncased"),  # WordPiece: may violate monotonicity
        ("openai_bpe", "gpt-3.5-turbo"),  # BPE: may violate monotonicity
    ):
        try:
            counter = make_counter(fam, name)

            # For subword tokenizers, test monotonicity with known problematic cases
            if fam in ("hf_fast", "openai_bpe"):
                # Test multiple problematic Unicode combinations
                test_cases = [
                    ("0A0Â¹", "Ë¥"),  # Mathematical symbols
                    ("a", "Ì"),  # Combining characters
                    ("Ã©", ""),  # Precomposed vs decomposed
                    ("ðŸ‘‹", "ðŸŒ"),  # Emoji sequences
                ]

                severe_violations = 0
                for test_a, test_b in test_cases:
                    ca = counter.count(test_a)
                    cb = counter.count(test_b)
                    ab = counter.count(test_a + test_b)

                    # Count severe violations (>50% reduction)
                    if ab < ca * 0.5 or ab < cb * 0.5:
                        severe_violations += 1

                # If more than half the test cases show severe violations, skip this tokenizer
                if severe_violations > len(test_cases) // 2:
                    print(
                        f"Warning: {fam} tokenizer shows {severe_violations}/{len(test_cases)} severe monotonicity violations, skipping"
                    )
                    continue

            return counter
        except Exception:
            continue
    return None


COUNTER = _maybe_counter()


# Determine if we're using a subword tokenizer that may have monotonicity violations
def _is_subword_tokenizer() -> bool:
    """Check if the current counter is a subword tokenizer with potential monotonicity issues."""
    if COUNTER is None:
        return False

    # Test with multiple problematic Unicode combinations to detect subword behavior
    test_cases = [
        ("0A0Â¹", "Ë¥"),  # Mathematical symbols
        ("a", "Ì"),  # Combining characters
        ("Ã©", ""),  # Precomposed vs decomposed
    ]

    violations = 0
    for test_a, test_b in test_cases:
        ca = COUNTER.count(test_a)
        cb = COUNTER.count(test_b)
        ab = COUNTER.count(test_a + test_b)
        # Count cases with significant violations (>30% reduction)
        if ab < ca * 0.7 or ab < cb * 0.7:
            violations += 1

    # If we see violations in most test cases, it's likely a subword tokenizer
    return violations >= len(test_cases) // 2


def _get_monotonicity_threshold() -> float:
    """Get appropriate monotonicity threshold based on tokenizer type."""
    if COUNTER is None:
        return 0.2

    is_subword = _is_subword_tokenizer()

    if is_subword:
        # For subword tokenizers, be more lenient due to fundamental limitations
        # Research shows BPE/WordPiece can compress sequences by up to 60% in extreme cases
        return 0.6
    else:
        # For character-level tokenizers, expect strict monotonicity
        return 0.2


@pytest.mark.prop
@pytest.mark.skipif(COUNTER is None, reason="no token counting backend available")
@given(s=st.text(max_size=400))
@settings(max_examples=20, deadline=100)
def test_unicode_normalization_non_negativity(s: str) -> None:
    """Test that Unicode normalization doesn't produce negative token counts."""
    n = COUNTER.count(s)
    if n < 0:
        record_case("test_unicode_normalization_nonneg", {"raw": s, "count": n})
    assert n >= 0


@pytest.mark.prop
@pytest.mark.skipif(COUNTER is None, reason="no token counting backend available")
@given(a=st.text(max_size=200), b=st.text(max_size=200))
@settings(max_examples=15, deadline=150)
def test_unicode_monotonic_concat(a: str, b: str) -> None:
    """Test monotonicity with Unicode normalization applied."""
    ca = COUNTER.count(a)
    cb = COUNTER.count(b)
    ab = COUNTER.count(a + b)

    # Record cases where monotonicity fails for analysis
    if ab < ca or ab < cb:
        record_case("test_unicode_monotonic_concat", {"a": a, "b": b, "ca": ca, "cb": cb, "cab": ab})

    # Use research-based thresholds for different tokenizer types
    max_reduction = _get_monotonicity_threshold()

    if ab < ca:
        reduction = (ca - ab) / ca if ca > 0 else 0
        if reduction > max_reduction:
            tokenizer_type = "subword" if _is_subword_tokenizer() else "character"
            assert (
                ab >= ca
            ), f"Severe token count reduction ({tokenizer_type}): {ca} -> {ab} (reduction: {reduction:.2%})"

    if ab < cb:
        reduction = (cb - ab) / cb if cb > 0 else 0
        if reduction > max_reduction:
            tokenizer_type = "subword" if _is_subword_tokenizer() else "character"
            assert (
                ab >= cb
            ), f"Severe token count reduction ({tokenizer_type}): {cb} -> {ab} (reduction: {reduction:.2%})"


# Unicode-specific edge cases that commonly cause monotonicity violations
UNICODE_EDGE_CASES = [
    # Combining characters
    ("a", "Ì"),  # a + combining acute
    ("e", "Ì€"),  # e + combining grave
    ("c", "Ì§"),  # c + combining cedilla
    # Precomposed vs decomposed
    ("Ã©", ""),  # precomposed e with acute
    ("e", "Ì"),  # decomposed e + combining acute
    # Emoji and special symbols
    ("ðŸ‘‹", "ðŸŒ"),  # emoji sequences
    ("â˜…", "â˜†"),  # star symbols
    ("â†’", "â†"),  # arrow symbols
    # Mathematical symbols
    ("âˆ‘", "âˆ"),  # summation and product
    ("Î±", "Î²"),  # Greek letters
    ("âˆž", "â‰ "),  # infinity and not equal
    # Currency symbols
    ("$", "â‚¬"),  # dollar and euro
    ("Â£", "Â¥"),  # pound and yen
    # Accented characters
    ("Ã±", "Ã¼"),  # Spanish and German
    ("Ã§", "ÅŸ"),  # French and Turkish
    # Zero-width characters
    ("a", "\u200b"),  # zero-width space
    ("b", "\u200c"),  # zero-width non-joiner
    ("c", "\u200d"),  # zero-width joiner
]


@pytest.mark.parametrize("a,b", UNICODE_EDGE_CASES)
@pytest.mark.skipif(COUNTER is None, reason="no token counting backend available")
def test_unicode_edge_cases_monotonicity(a: str, b: str) -> None:
    """Test monotonicity with specific Unicode edge cases."""
    ca = COUNTER.count(a)
    cb = COUNTER.count(b)
    ab = COUNTER.count(a + b)

    # Record violations for analysis
    if ab < ca or ab < cb:
        record_case("test_unicode_edge_cases", {"a": repr(a), "b": repr(b), "ca": ca, "cb": cb, "cab": ab})

    # With Unicode normalization, these should mostly pass
    # Allow some tolerance for complex Unicode sequences
    if ab < ca:
        reduction = (ca - ab) / ca if ca > 0 else 0
        if reduction > 0.3:  # More than 30% reduction (realistic threshold)
            assert (
                ab >= ca
            ), f"Unicode edge case violation: {repr(a)} + {repr(b)} -> {ca} -> {ab} (reduction: {reduction:.2%})"

    if ab < cb:
        reduction = (cb - ab) / cb if cb > 0 else 0
        if reduction > 0.3:  # More than 30% reduction (realistic threshold)
            assert (
                ab >= cb
            ), f"Unicode edge case violation: {repr(a)} + {repr(b)} -> {cb} -> {ab} (reduction: {reduction:.2%})"


@pytest.mark.prop
@pytest.mark.skipif(COUNTER is None, reason="no token counting backend available")
@given(
    text=st.text(
        alphabet=st.characters(
            min_codepoint=0x0000,
            max_codepoint=0x10FFFF,
            blacklist_categories=("Cc", "Cf", "Cs"),  # Exclude control, format, surrogate chars
        ),
        max_size=100,
    )
)
@settings(max_examples=10, deadline=200)
def test_unicode_range_monotonicity(text: str) -> None:
    """Test monotonicity across various Unicode ranges."""
    if not text:  # Skip empty strings
        return

    # Split text roughly in half
    mid = len(text) // 2
    a, b = text[:mid], text[mid:]

    if not a or not b:  # Skip if split results in empty strings
        return

    ca = COUNTER.count(a)
    cb = COUNTER.count(b)
    ab = COUNTER.count(a + b)

    # Record violations
    if ab < ca or ab < cb:
        record_case(
            "test_unicode_range_monotonicity",
            {"text": repr(text), "a": repr(a), "b": repr(b), "ca": ca, "cb": cb, "cab": ab},
        )

    # Use research-based thresholds for different tokenizer types
    max_reduction = _get_monotonicity_threshold()

    if ab < ca:
        reduction = (ca - ab) / ca if ca > 0 else 0
        if reduction > max_reduction:
            tokenizer_type = "subword" if _is_subword_tokenizer() else "character"
            assert ab >= ca, f"Unicode range violation ({tokenizer_type}): {ca} -> {ab} (reduction: {reduction:.2%})"

    if ab < cb:
        reduction = (cb - ab) / cb if cb > 0 else 0
        if reduction > max_reduction:
            tokenizer_type = "subword" if _is_subword_tokenizer() else "character"
            assert ab >= cb, f"Unicode range violation ({tokenizer_type}): {cb} -> {ab} (reduction: {reduction:.2%})"
