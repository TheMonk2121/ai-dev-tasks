# Evaluation Manifest Template
# Save one per run alongside results. Makes every eval explainable and diff-able.

run_id: ${INGEST_RUN_ID}
config_hash: ${CONFIG_HASH}
timestamp: ${TIMESTAMP}

chunking:
  version: 2025-09-07-v1
  chunk_size: 450
  overlap_ratio: 0.10
  jaccard_threshold: 0.8
  prefix_policy: A        # embedding-only

models:
  embedder: intfloat/e5-large-v2
  embed_dim: 1024         # or 384 if you switched
  reranker: BAAI/bge-reranker-base
  generator: <name-or-bedrock-id>

retrieval:
  rrf_k: 60
  topk_vec: 140
  topk_bm25: 140
  rerank_pool: 60
  rerank_topn: 18
  context_docs_max: 12

eval:
  temperature: 0
  few_shot_k: 0           # baseline pass; second pass set to 5
  cot_enabled: false
  cache_disabled: true
  dataset_id: <dev|test|holdout>
  thresholds:
    f1_min: <baseline>
    precision_drift_max: 0.02
    latency_p95_budget: "+15%"
    oracle_prefilter_target: 0.85

provenance:
  prompt_audit: true
  fields: [prompt_hash, few_shot_ids, prompt_tokens]
  table: document_chunks_2025_09_07_040048_v1

# Generated by: scripts/eval_manifest_generator.py
# Template version: 1.0
