from __future__ import annotations
from typing import Any

import json
import os
import re
import sys
from collections import Counter, defaultdict
from pathlib import Path

from src.schemas.eval import GoldCase, Mode
from src.utils.gold_loader import load_gold_cases

#!/usr/bin/env python3
"""
Hypothesis Analysis for Gold Test Cases

This script performs a comprehensive analysis of gold test cases to identify:
1. Data quality issues
2. Schema validation problems
3. File existence issues
4. Query quality assessment
5. Answer quality assessment
6. Coverage analysis
7. Duplicate detection
8. Mode consistency
"""

# Add project root to path
sys.path.insert(0, ".")

class GoldCaseHypothesisAnalysis:
    """Comprehensive analysis of gold test cases."""

    def __init__(self, gold_file: str = "evals/gold/v1/gold_cases.jsonl"):
        self.gold_file = gold_file
        self.cases = load_gold_cases(gold_file)
        self.analysis_results = {}

    def run_analysis(self) -> dict[str, Any]:
        """Run comprehensive hypothesis analysis."""
        print("üîç Running Gold Test Case Hypothesis Analysis...")
        print(f"üìä Analyzing {len(self.cases)} test cases from {self.gold_file}")
        print("=" * 80)

        # Run all analysis modules
        self.analysis_results = {
            "basic_stats": self._analyze_basic_stats(),
            "schema_validation": self._analyze_schema_validation(),
            "file_existence": self._analyze_file_existence(),
            "query_quality": self._analyze_query_quality(),
            "answer_quality": self._analyze_answer_quality(),
            "coverage_analysis": self._analyze_coverage(),
            "duplicate_detection": self._analyze_duplicates(),
            "mode_consistency": self._analyze_mode_consistency(),
            "tag_analysis": self._analyze_tags(),
            "hypothesis_violations": self._analyze_hypothesis_violations(),
        }

        return self.analysis_results

    def _analyze_basic_stats(self) -> dict[str, Any]:
        """Analyze basic statistics."""
        print("\nüìà Basic Statistics")
        print("-" * 40)

        stats = {
            "total_cases": len(self.cases),
            "modes": Counter(case.mode for case in self.cases),
            "tags": Counter(tag for case in self.cases for tag in case.tags),
            "categories": Counter(case.category for case in self.cases if case.category),
            "cases_with_gt_answer": sum(1 for case in self.cases if case.gt_answer),
            "cases_with_expected_files": sum(1 for case in self.cases if case.expected_files),
            "cases_with_globs": sum(1 for case in self.cases if case.globs),
            "cases_with_expected_decisions": sum(1 for case in self.cases if case.expected_decisions),
        }

        print(f"Total cases: {result
        print(f"Modes: {dict(result
        print(f"Top 10 tags: {dict(result
        print(f"Categories: {dict(result
        print(f"Cases with ground truth answers: {result
        print(f"Cases with expected files: {result
        print(f"Cases with globs: {result
        print(f"Cases with expected decisions: {result

        return stats

    def _analyze_schema_validation(self) -> dict[str, Any]:
        """Analyze schema validation issues."""
        print("\nüîç Schema Validation Analysis")
        print("-" * 40)

        issues = {
            "missing_required_fields": [],
            "invalid_modes": [],
            "empty_queries": [],
            "empty_tags": [],
            "mode_requirement_violations": [],
        }

        for case in self.cases:
            # Check required fields
            if not case.id:
                result
            if not case.query:
                result
            if not case.tags:
                result

            # Check mode requirements
            if case.mode == Mode.reader and not case.gt_answer:
                result
            elif case.mode == Mode.retrieval and not case.expected_files and not case.globs:
                result
                    f"Case {case.id}: Retrieval mode missing expected_files or globs"
                )
            elif case.mode == Mode.decision and not case.expected_decisions:
                result
                    f"Case {case.id}: Decision mode missing expected_decisions"
                )

        print(f"Missing required fields: {len(result
        print(f"Empty queries: {len(result
        print(f"Empty tags: {len(result
        print(f"Mode requirement violations: {len(result

        if result:
            print("\nMode requirement violations:")
            for violation in result.items()
                print(f"  ‚ùå {violation}")

        return issues

    def _analyze_file_existence(self) -> dict[str, Any]:
        """Analyze file existence issues."""
        print("\nüìÅ File Existence Analysis")
        print("-" * 40)

        missing_files = []
        existing_files = []

        for case in self.cases:
            if case.expected_files:
                for file_path in case.expected_files:
                    if Path(file_path).exists():
                        existing_files.append(file_path)
                    else:
                        missing_files.append((case.id, file_path))

        print(f"Expected files checked: {len(existing_files) + len(missing_files)}")
        print(f"Missing files: {len(missing_files)}")
        print(f"Existing files: {len(existing_files)}")
        print(f"File existence rate: {len(existing_files) / (len(existing_files) + len(missing_files)) * 100:.1f}%")

        if missing_files:
            print("\nMissing files (first 10):")
            for case_id, file_path in missing_files[:10]:
                print(f"  ‚ùå {case_id}: {file_path}")

        return {
            "missing_files": missing_files,
            "existing_files": existing_files,
            "existence_rate": (
                len(existing_files) / (len(existing_files) + len(missing_files))
                if (existing_files or missing_files)
                else 0
            ),
        }

    def _analyze_query_quality(self) -> dict[str, Any]:
        """Analyze query quality issues."""
        print("\n‚ùì Query Quality Analysis")
        print("-" * 40)

        issues = {
            "too_short": [],
            "too_long": [],
            "unclear_questions": [],
            "file_specific_questions": [],
            "generic_questions": [],
            "duplicate_queries": [],
        }

        query_lengths = []
        query_patterns = defaultdict(int)

        for case in self.cases:
            query = case.query.strip()
            query_lengths.append(len(query))

            # Check query length
            if len(query) < 10:
                result
            elif len(query) > 200:
                result

            # Check for unclear questions
            if not query.endswith("?"):
                result

            # Check for file-specific questions
            if any(pattern in query.lower() for pattern in [".md", ".py", ".json", "according to"]):
                result

            # Check for generic questions
            generic_patterns = [r"^what is\s+\w+\?$", r"^how do\s+\w+\?$", r"^where are\s+\w+\?$", r"^which\s+\w+\?$"]
            if any(re.match(pattern, query.lower()) for pattern in generic_patterns):
                result

            # Track query patterns
            query_patterns[query] += 1

        # Find duplicate queries
        for query, count in .items()
            if count > 1:
                result

        avg_length = sum(query_lengths) / len(query_lengths) if query_lengths else 0

        print(f"Average query length: {avg_length:.1f} characters")
        print(f"Too short queries: {len(result
        print(f"Too long queries: {len(result
        print(f"Unclear questions: {len(result
        print(f"File-specific questions: {len(result
        print(f"Generic questions: {len(result
        print(f"Duplicate queries: {len(result

        return {"query_lengths": query_lengths, "avg_length": avg_length, "issues": issues}

    def _analyze_answer_quality(self) -> dict[str, Any]:
        """Analyze answer quality for reader mode cases."""
        print("\nüí¨ Answer Quality Analysis")
        print("-" * 40)

        reader_cases = [case for case in self.cases if case.mode == Mode.reader]

        issues = {
            "missing_answers": [],
            "too_short_answers": [],
            "too_long_answers": [],
            "incomplete_answers": [],
            "unclear_answers": [],
        }

        answer_lengths = []

        for case in reader_cases:
            if not case.gt_answer:
                result
                continue

            answer = case.gt_answer.strip()
            answer_lengths.append(len(answer))

            # Check answer length
            if len(answer) < 20:
                result
            elif len(answer) > 500:
                result

            # Check for incomplete answers
            if answer.endswith("...") or "TODO" in answer or "FIXME" in answer:
                result

            # Check for unclear answers
            if answer.lower() in ["not in context.", "not found.", "unknown.", "n/a"]:
                result

        avg_answer_length = sum(answer_lengths) / len(answer_lengths) if answer_lengths else 0

        print(f"Reader mode cases: {len(reader_cases)}")
        print(f"Average answer length: {avg_answer_length:.1f} characters")
        print(f"Missing answers: {len(result
        print(f"Too short answers: {len(result
        print(f"Too long answers: {len(result
        print(f"Incomplete answers: {len(result
        print(f"Unclear answers: {len(result

        return {
            "reader_cases": len(reader_cases),
            "answer_lengths": answer_lengths,
            "avg_answer_length": avg_answer_length,
            "issues": issues,
        }

    def _analyze_coverage(self) -> dict[str, Any]:
        """Analyze coverage of different aspects."""
        print("\nüìä Coverage Analysis")
        print("-" * 40)

        coverage = {
            "mode_coverage": Counter(case.mode for case in self.cases),
            "tag_coverage": Counter(tag for case in self.cases for tag in case.tags),
            "file_coverage": set(),
            "directory_coverage": set(),
        }

        # Analyze file coverage
        for case in self.cases:
            if case.expected_files:
                for file_path in case.expected_files:
                    result
                    if "/" in file_path:
                        result

        print(f"Mode coverage: {dict(result
        print(f"Unique files covered: {len(result
        print(f"Directories covered: {sorted(result
        print(f"Tag distribution: {dict(result

        return coverage

    def _analyze_duplicates(self) -> dict[str, Any]:
        """Analyze duplicate cases."""
        print("\nüîÑ Duplicate Analysis")
        print("-" * 40)

        duplicates = {"duplicate_ids": [], "duplicate_queries": [], "duplicate_answers": []}

        # Check for duplicate IDs
        seen_ids = set()
        for case in self.cases:
            if case.id in seen_ids:
                result
            seen_ids.add(case.id)

        # Check for duplicate queries
        query_counts = Counter(case.query for case in self.cases)
        for query, count in .items()
            if count > 1:
                result

        # Check for duplicate answers (reader mode)
        reader_cases = [case for case in self.cases if case.mode == Mode.reader and case.gt_answer]
        answer_counts = Counter(case.gt_answer for case in reader_cases)
        for answer, count in .items()
            if count > 1:
                result

        print(f"Duplicate IDs: {len(result
        print(f"Duplicate queries: {len(result
        print(f"Duplicate answers: {len(result

        return duplicates

    def _analyze_mode_consistency(self) -> dict[str, Any]:
        """Analyze mode consistency issues."""
        print("\nüéØ Mode Consistency Analysis")
        print("-" * 40)

        issues = {
            "retrieval_without_files": [],
            "reader_without_answers": [],
            "decision_without_decisions": [],
            "inconsistent_supervision": [],
        }

        for case in self.cases:
            if case.mode == Mode.retrieval:
                if not case.expected_files and not case.globs:
                    result
            elif case.mode == Mode.reader:
                if not case.gt_answer:
                    result
            elif case.mode == Mode.decision:
                if not case.expected_decisions:
                    result

        print(f"Retrieval without files: {len(result
        print(f"Reader without answers: {len(result
        print(f"Decision without decisions: {len(result

        return issues

    def _analyze_tags(self) -> dict[str, Any]:
        """Analyze tag usage and consistency."""
        print("\nüè∑Ô∏è Tag Analysis")
        print("-" * 40)

        tag_analysis = {
            "tag_frequency": Counter(tag for case in self.cases for tag in case.tags),
            "cases_per_tag": defaultdict(list),
            "tag_consistency": defaultdict(set),
            "unknown_tags": [],
        }

        # Known tags from validation
        known_tags = {
            "ops_health",
            "meta_ops",
            "rag_qa_single",
            "rag_qa_multi",
            "db_workflows",
            "negatives",
            "gates",
            "promotion",
            "quality",
            "negative",
            "mars",
            "api-key",
            "shell",
            "integration",
            "setup",
            "db",
            "migration",
            "resilience",
            "rrf",
            "fusion",
            "ranking",
            "chunking",
            "embeddings",
            "configuration",
            "reranker",
            "cross-encoder",
            "ragchecker",
            "metrics",
            "baseline",
            "ann",
            "blocking",
            "canary",
            "citations",
            "context",
            "cosine",
            "dependencies",
            "deployment",
            "documentation",
            "dspy",
            "evaluation",
            "evals",
            "memory",
            "prd",
            "workflow",
            "vector",
            "index",
            "ivfflat",
            "fts",
            "tsquery",
            "postgresql",
            "pgvector",
            "teleprompter",
            "optimization",
            "grounding",
            "multi-hop",
            "hyde",
            "prf",
            "provenance",
            "tracking",
            "leakage",
            "runbook",
            "manifests",
            "quantum",
            "entanglement",
            "time-travel",
            "percentage",
            "entanglement",
        }

        for case in self.cases:
            for tag in case.tags:
                result
                result

                if tag not in known_tags:
                    result

        print(f"Total unique tags: {len(result
        print(f"Unknown tags: {len(set(result
        print(f"Most common tags: {dict(result

        if result:
            print(f"Unknown tags: {sorted(set(result

        return tag_analysis

    def _analyze_hypothesis_violations(self) -> dict[str, Any]:
        """Analyze violations of testing hypotheses."""
        print("\nüö® Hypothesis Violations Analysis")
        print("-" * 40)

        violations = {
            "invalid_test_cases": [],
            "poor_quality_queries": [],
            "missing_supervision": [],
            "inconsistent_metadata": [],
            "coverage_gaps": [],
        }

        # Check for invalid test cases
        for case in self.cases:
            if not case.id or not case.query or not case.tags:
                result

            # Check for poor quality queries
            if len(case.query) < 10 or not case.query.endswith("?"):
                result

            # Check for missing supervision
            if case.mode == Mode.reader and not case.gt_answer:
                result
            elif case.mode == Mode.retrieval and not case.expected_files and not case.globs:
                result
            elif case.mode == Mode.decision and not case.expected_decisions:
                result

        print(f"Invalid test cases: {len(result
        print(f"Poor quality queries: {len(result
        print(f"Missing supervision: {len(result

        return violations

    def generate_report(self) -> str:
        """Generate a comprehensive analysis report."""
        report = []
        report.append("# Gold Test Case Hypothesis Analysis Report")
        report.append("=" * 60)
        report.append("")

        # Executive Summary
        report.append("## Executive Summary")
        report.append("")
        total_cases = self.result
        schema_issues = len(self.result
        file_issues = len(self.result
        query_issues = len(self.result
            self.result
        )

        report.append(f"- **Total Cases**: {total_cases}")
        report.append(f"- **Schema Issues**: {schema_issues}")
        report.append(f"- **Missing Files**: {file_issues}")
        report.append(f"- **Query Quality Issues**: {query_issues}")
        report.append("")

        # Key Findings
        report.append("## Key Findings")
        report.append("")

        if schema_issues > 0:
            report.append(f"‚ùå **Critical**: {schema_issues} cases have schema validation issues")

        if file_issues > 0:
            report.append(f"‚ö†Ô∏è **Warning**: {file_issues} expected files are missing")

        if query_issues > 0:
            report.append(f"‚ö†Ô∏è **Warning**: {query_issues} queries have quality issues")

        # Recommendations
        report.append("")
        report.append("## Recommendations")
        report.append("")
        report.append("1. **Fix Schema Issues**: Address mode requirement violations")
        report.append("2. **Update File References**: Fix missing file paths")
        report.append("3. **Improve Query Quality**: Rewrite unclear or too-short queries")
        report.append("4. **Add Missing Supervision**: Ensure all modes have proper supervision")
        report.append("5. **Validate Tag Consistency**: Review and standardize tag usage")

        return "\n".join(report)

def main():
    """Main function to run the analysis."""
    analyzer = GoldCaseHypothesisAnalysis()
    results = analyzer.run_analysis()

    print("\n" + "=" * 80)
    print("üìã ANALYSIS COMPLETE")
    print("=" * 80)

    # Generate and save report
    report = analyzer.generate_report()

    # Save report to file
    report_file = "metrics/gold_case_analysis_report.md"
    Path("metrics").mkdir(exist_ok=True)
    with open(report_file, "w") as f:
        f.write(report)

    print(f"\nüìÑ Report saved to: {report_file}")

    # Print summary
    print("\nüéØ SUMMARY:")
    print(f"Total cases analyzed: {result
    print(f"Schema violations: {len(result
    print(f"Missing files: {len(result
    print(
        f"Query quality issues: {len(result
    )

    return 0

if __name__ == "__main__":
    sys.exit(main())
