{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "501b8382",
      "metadata": {},
      "source": [
        "# Evaluation Ledger Analysis Dashboard\n",
        "\n",
        "This notebook aligns with the production evaluation pipeline by reusing\n",
        "the shared helpers that power `evals/scripts/evaluation/core/production_evaluation.py`.\n",
        "It is designed to be executed locally with the same configuration gates\n",
        "used by the CLI workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8fa6bd4",
      "metadata": {},
      "source": [
        "## Usage\n",
        "1. Resolve the repository root and import the shared helpers.\n",
        "2. Inspect the configured passes to confirm the environment deltas.\n",
        "3. Run a dry-run to verify wiring or execute the passes for real metrics.\n",
        "4. Open the generated JSON artifacts under `metrics/production_evaluations`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cfa6e4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from collections.abc import Iterable\n",
        "from pathlib import Path\n",
        "\n",
        "from evals.scripts.evaluation.core.production_eval_helpers import (\n",
        "    DEFAULT_PRODUCTION_PASSES,\n",
        "    ProductionEvaluationSummary,\n",
        "    run_production_evaluation,\n",
        ")\n",
        "\n",
        "\n",
        "def resolve_project_root(candidates: Iterable[Path] | None = None) -> Path:\n",
        "    \"\"\"Best-effort resolution of the repository root for notebook execution.\"\"\"\n",
        "\n",
        "    probe = list(candidates or [])\n",
        "    if not probe:\n",
        "        cwd = Path.cwd().resolve()\n",
        "        probe = [cwd, cwd.parent, cwd.parents[1] if len(cwd.parents) > 1 else cwd]\n",
        "\n",
        "    for candidate in probe:\n",
        "        marker = candidate / \"pyproject.toml\"\n",
        "        if marker.exists():\n",
        "            return candidate\n",
        "    raise RuntimeError(\"Unable to locate project root; add it to candidates.\")\n",
        "\n",
        "\n",
        "PROJECT_ROOT = resolve_project_root()\n",
        "RESULTS_DIR = PROJECT_ROOT / \"metrics/production_evaluations\"\n",
        "PROJECT_ROOT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6865cbb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "passes = DEFAULT_PRODUCTION_PASSES\n",
        "for idx, cfg in enumerate(passes, start=1):\n",
        "    print(f\"Pass {idx}: {cfg.name}\")\n",
        "    print(f\"  Description: {cfg.description}\")\n",
        "    for key, value in cfg.env.items():\n",
        "        print(f\"    {key}={value}\")\n",
        "    if cfg.extra_args:\n",
        "        print(f\"    extra_args={cfg.extra_args}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82237369",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dry-run to confirm everything resolves (set execute=True to run the full evaluation).\n",
        "summary: ProductionEvaluationSummary = run_production_evaluation(\n",
        "    passes,\n",
        "    project_root=PROJECT_ROOT,\n",
        "    results_dir=RESULTS_DIR,\n",
        "    execute=False,\n",
        "    capture_output=False,\n",
        ")\n",
        "summary.overall_status\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8567f61c",
      "metadata": {},
      "source": [
        "When you are ready to execute the real evaluation, rerun the previous cell\n",
        "with `execute=True`. The helper will write per-pass JSON payloads and an\n",
        "aggregate analysis file (`analysis_<timestamp>.json`) under the metrics\n",
        "directory."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
