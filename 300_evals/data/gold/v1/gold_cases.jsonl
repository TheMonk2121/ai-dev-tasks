{"id":"GOLD_MEMORY_001","mode":"reader","query":"What is the current status of my memory system?","tags":["memory","status","system"],"category":"ops_health","gt_answer":"Memory system is PRODUCTION READY with Lessons Engine fully implemented. Key components: lessons_extractor.py, lessons_loader.py, evolution_tracker.py, lessons_quality_check.py. Full integration with ragchecker_official_evaluation.py. System ready for evaluation runs with automatic lesson learning.","expected_files":["100_memory/100_cursor-memory-context.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_MEMORY_002","mode":"reader","query":"How does my memory system work?","tags":["memory","architecture","workflow"],"category":"ops_health","gt_answer":"Memory system provides context rehydration, knowledge persistence, cross-session continuity, and decision intelligence. Key components: LTST Memory System for long-term storage, Cursor Memory Context for IDE-specific context, DSPy Role System for AI agent roles, Backlog Integration for task management. Uses PostgreSQL + pgvector for storage.","expected_files":["100_memory/100_cursor-memory-context.md","400_guides/400_00_memory-system-overview.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_MEMORY_003","mode":"reader","query":"How do you run the memory system health check?","tags":["memory","health","check"],"category":"ops_health","gt_answer":"Run: export POSTGRES_DSN=\"mock://test\" && uv run python scripts/unified_memory_orchestrator.py --systems ltst cursor go_cli prime --role planner \"current project status and core documentation\". Also use: uv run python scripts/lessons_quality_check.py to verify system health.","expected_files":["100_memory/100_cursor-memory-context.md","scripts/unified_memory_orchestrator.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_MEMORY_004","mode":"reader","query":"What is my LTST memory system?","tags":["memory","ltst","system"],"category":"ops_health","gt_answer":"LTST (Long-Term Short-Term) Memory System provides conversation persistence, session tracking, context merging, and automatic memory rehydration. Extends PostgreSQL + pgvector infrastructure to store conversation history, session metadata, and context relationships. Enables seamless conversation continuity across AI sessions.","expected_files":["000_core/development artifacts/PRD-B-1012-LTST-Memory-System.md","scripts/unified_memory_orchestrator.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_DSPY_001","mode":"reader","query":"What local models do I use for my DSPy?","tags":["dspy","models","local"],"category":"ops_health","gt_answer":"Two local models: Mistral 7B Instruct (warm, always resident, ~8GB) for planning and reasoning tasks, and Yi-Coder-9B-Chat-Q6_K (lazy, load on demand, ~19GB) for code generation. Both run via LM Studio on localhost:1234 with Ollama on localhost:11434.","expected_files":["200_setup/201_model-configuration.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_DSPY_002","mode":"reader","query":"What is DSPy?","tags":["dspy","framework","ai"],"category":"ops_health","gt_answer":"DSPy is a framework for programming with foundation models. It provides a systematic approach to building and optimizing AI systems with automatic prompt optimization, few-shot learning, and model selection. Used in this project for RAG pipeline optimization and multi-agent coordination.","expected_files":["400_guides/400_09_ai-frameworks-dspy.md","100_memory/104_dspy-development-context.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_DB_001","mode":"reader","query":"What do I use for my database?","tags":["database","postgresql","storage"],"category":"db_workflows","gt_answer":"PostgreSQL 15+ with pgvector extension for vector operations. Database: ai_agency on localhost:5432. Uses connection pooling, backup strategy, and advanced features like PostgreSQL functions for context merging and memory rehydration.","expected_files":["200_setup/201_database-config.py","configs/profiles/real.env"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_DB_002","mode":"reader","query":"What extensions do I have in my database?","tags":["database","extensions","postgresql"],"category":"db_workflows","gt_answer":"Required extensions: vector (pgvector) ≥0.8 for production, pg_trgm for text similarity, pg_stat_statements for performance visibility. Uses GIN index for tsvector columns and HNSW/IVFFlat indexes for vector operations.","expected_files":["200_setup/201_database-config.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_DB_003","mode":"reader","query":"What are our chunking settings?","tags":["chunking","configuration","embeddings"],"category":"db_workflows","gt_answer":"Default chunk size: 450 tokens, overlap ratio: 0.10, Jaccard threshold: 0.8 for deduplication, prefix policy: A (embedding-only). Uses intfloat/e5-large-v2 embedder with 1024 embedding dimensions.","expected_files":["templates/eval_manifest_template.yaml","300_evals/configs/yaml/base.yaml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_DB_004","mode":"reader","query":"What are our embedding settings?","tags":["embeddings","configuration","vector"],"category":"db_workflows","gt_answer":"Embedder: intfloat/e5-large-v2 (or BAAI/bge-large-en-v1.5), embedding dimensions: 1024, reranker: BAAI/bge-reranker-base. Uses pgvector with cosine distance operator (<=>) and HNSW/IVFFlat indexes for approximate nearest neighbor search.","expected_files":["templates/eval_manifest_template.yaml","300_evals/configs/yaml/base.yaml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_TOOLS_001","mode":"reader","query":"What do I use as a type checker?","tags":["type-checking","tools","quality"],"category":"ops_health","gt_answer":"Preferred: basedpyright (drop-in, faster, stricter typing). Fallback: pyright. CI runs: uv run basedpyright || uv run pyright. Configuration: strict type checking mode, Python 3.12, reportMissingTypeStubs=false.","expected_files":["000_core/001_PRD_TEMPLATE.md",".github/workflows/eval.yml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_TOOLS_002","mode":"reader","query":"What are my two venv and what are their purposes?","tags":["venv","environments","development"],"category":"ops_health","gt_answer":"Two virtual environments: .venv (local macOS development with dev extras) and /opt/venv (Docker/CI Linux environment, installs from uv.lock only). Local uses UV_PROJECT_ENVIRONMENT=.venv, CI uses UV_PROJECT_ENVIRONMENT=/opt/venv.","expected_files":["scripts/unified_memory_orchestrator.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_DOCS_001","mode":"reader","query":"What are the 13 core 400_ documents?","tags":["documentation","guides","structure"],"category":"ops_health","gt_answer":"13 core 400_ guides: 00_memory-system-overview, 01_memory-system-architecture, 02_memory-rehydration-context-management, 03_system-overview-and-architecture, 04_development-workflow-and-standards, 05_codebase-organization-patterns, 06_backlog-management-priorities, 07_project-planning-roadmap, 08_task-management-workflows, 09_ai-frameworks-dspy, 10_integrations-models, 11_performance-optimization, 12_advanced-configurations.","expected_files":["400_guides/400_00_memory-system-overview.md","400_guides/400_03_system-overview-and-architecture.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_DOCS_002","mode":"reader","query":"What is the purpose of 001_PRD_TEMPLATE?","tags":["prd","template","workflow"],"category":"ops_health","gt_answer":"001_PRD_TEMPLATE.md is used to create Product Requirements Documents following the established workflow. It provides a structured template for documenting project requirements, acceptance criteria, and implementation details. Part of the core workflow chain: backlog → PRD → tasks → execution.","expected_files":["000_core/001_PRD_TEMPLATE.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_DOCS_003","mode":"reader","query":"What is the purpose of 002_TASK-LIST_TEMPLATE?","tags":["tasks","template","workflow"],"category":"ops_health","gt_answer":"002_TASK-LIST_TEMPLATE.md is used to generate task lists from PRDs. It provides a structured template for breaking down requirements into actionable tasks with priorities, dependencies, and acceptance criteria. Part of the core workflow chain: PRD → tasks → execution.","expected_files":["000_core/002_TASK-LIST_TEMPLATE.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_DOCS_004","mode":"reader","query":"What is the purpose of 003_EXECUTION_TEMPLATE?","tags":["execution","template","workflow"],"category":"ops_health","gt_answer":"003_EXECUTION_TEMPLATE.md is used for AI execution workflows. It provides a structured template for implementing tasks, tracking progress, and managing the execution phase. Part of the core workflow chain: tasks → execution → completion.","expected_files":["000_core/003_EXECUTION_TEMPLATE.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_EVAL_001","mode":"reader","query":"What are the current eval questions?","tags":["evaluation","questions","gold"],"category":"ops_health","gt_answer":"Current eval questions are stored in 300_evals/data/gold/v1/gold_cases.jsonl with 122 cases covering retrieval, reader, and decision modes. Categories include ops_health, db_workflows, rag_qa_single, rag_qa_multi, meta_ops, and negatives.","expected_files":["300_evals/data/gold/v1/gold_cases.jsonl"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_EVAL_002","mode":"reader","query":"What is the structure and format for creating new gold evaluation questions?","tags":["evaluation","format","structure"],"category":"ops_health","gt_answer":"Gold evaluation questions use JSON format with required fields: id (unique identifier), mode (retrieval/reader/decision), query (question text), tags (categories), expected_files (relevant files), gt_answer (ground truth for reader mode), assertions (quality requirements). Stored in 300_evals/data/gold/v1/gold_cases.jsonl.","expected_files":["300_evals/data/gold/v1/gold_cases.jsonl","400_guides/400_11_performance-optimization.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_EVAL_003","mode":"reader","query":"What are the 3 main modules for my eval system?","tags":["evaluation","modules","system"],"category":"ops_health","gt_answer":"Three main evaluation modules: 1) Pydantic Evals framework (300_evals/evals/) with type-safe evaluation and Logfire observability, 2) RAGChecker evaluation system (scripts/ragchecker_official_evaluation.py) for RAG quality assessment, 3) Legacy evaluation system (evals_300/) with auto-generated documentation.","expected_files":["300_evals/evals/","scripts/ragchecker_official_evaluation.py","evals_300/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_EVAL_004","mode":"reader","query":"What are the required fields for eval questions?","tags":["evaluation","fields","schema"],"category":"ops_health","gt_answer":"Required fields: id (string, unique), mode (retrieval/reader/decision), query (string), tags (array of strings). Optional fields: category, gt_answer (reader mode), expected_files (retrieval/decision), globs, expected_decisions (decision mode), notes. Must have at least one of expected_files, globs, gt_answer, or expected_decisions.","expected_files":["300_evals/data/gold/v1/gold_cases.jsonl","400_guides/400_11_performance-optimization.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_EVAL_005","mode":"reader","query":"What is Pydantic?","tags":["pydantic","framework","validation"],"category":"ops_health","gt_answer":"Pydantic is a Python library for data validation and settings management using Python type annotations. It provides automatic data validation, serialization, and error handling. Used in this project for type-safe evaluation contracts, configuration management, and data validation throughout the codebase.","expected_files":["300_evals/evals/qa_eval.py","300_evals/evals/retrieval_eval.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_EVAL_006","mode":"reader","query":"What is pydantic logfire?","tags":["pydantic","logfire","observability"],"category":"ops_health","gt_answer":"Pydantic Logfire is an observability and monitoring tool that provides structured logging, metrics, and tracing for Pydantic-based applications. It offers automatic instrumentation, performance monitoring, and debugging capabilities. Used in the evaluation system for observability and performance tracking.","expected_files":["300_evals/evals/qa_eval.py","300_evals/evals/retrieval_eval.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_EVAL_007","mode":"reader","query":"What purpose does pydantic logfire serve in our evals?","tags":["pydantic","logfire","evaluation"],"category":"ops_health","gt_answer":"Pydantic Logfire serves as the observability layer for the evaluation system, providing structured logging, performance monitoring, and debugging capabilities. It enables tracking of evaluation metrics, performance bottlenecks, and system health. Used in 300_evals/evals/ for comprehensive evaluation monitoring.","expected_files":["300_evals/evals/qa_eval.py","300_evals/evals/retrieval_eval.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_EVAL_008","mode":"reader","query":"Where are our evals stored?","tags":["evaluation","storage","location"],"category":"ops_health","gt_answer":"Evaluations are stored in multiple locations: 1) Gold cases in 300_evals/data/gold/v1/gold_cases.jsonl, 2) Evaluation results in metrics/baseline_evaluations/, 3) Pydantic Evals framework in 300_evals/evals/, 4) Legacy evaluation system in evals_300/. Results include ABP files and context metadata sidecars.","expected_files":["300_evals/data/gold/v1/gold_cases.jsonl","metrics/baseline_evaluations/","300_evals/evals/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_EVAL_009","mode":"reader","query":"How do I run the evaluation tests?","tags":["evaluation","testing","commands"],"category":"ops_health","gt_answer":"Run evaluations using: 1) Pydantic Evals: python3 scripts/pydantic_evals_integration.py, 2) RAGChecker: uv run python scripts/ragchecker_official_evaluation.py --use-bedrock --bypass-cli --stable --lessons-mode advisory --lessons-scope profile --lessons-window 5, 3) Legacy: python -m evals_300.tools.run --suite 300_core --pass retrieval_only_baseline.","expected_files":["000_core/000_evaluation-system-entry-point.md","scripts/ragchecker_official_evaluation.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_EVAL_010","mode":"reader","query":"How do I configure the reranker?","tags":["reranker","configuration","rag"],"category":"rag_qa_single","gt_answer":"Set RERANK_ENABLE=1 and use BAAI/bge-reranker-base model for cross-encoder reranking with configurable pool size. Configure via environment variables or config files. Used in the RAG pipeline for improving retrieval quality through reranking.","expected_files":["src/dspy_modules/rag_pipeline.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_WORKFLOW_001","mode":"reader","query":"How do you add an item to the backlog?","tags":["backlog","workflow","management"],"category":"ops_health","gt_answer":"Add items to the backlog by editing 000_core/000_backlog.md. Follow the established format with priority levels, point estimates, dependencies, and acceptance criteria. Use the backlog management system to track progress and maintain project priorities.","expected_files":["000_core/000_backlog.md","400_guides/400_06_backlog-management-priorities.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_WORKFLOW_002","mode":"reader","query":"What is my backlog?","tags":["backlog","priorities","project"],"category":"ops_health","gt_answer":"The backlog is stored in 000_core/000_backlog.md and contains current project priorities, active development items, and development roadmap. It shows blocking dependencies and helps identify urgent vs. waiting tasks. Essential for understanding project direction and next steps.","expected_files":["000_core/000_backlog.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_WORKFLOW_003","mode":"reader","query":"What are PRDs?","tags":["prd","requirements","documentation"],"category":"ops_health","gt_answer":"PRDs (Product Requirements Documents) are structured documents that define project requirements, acceptance criteria, and implementation details. Created using 001_PRD_TEMPLATE.md following the established workflow. Part of the core workflow chain: backlog → PRD → tasks → execution.","expected_files":["000_core/001_PRD_TEMPLATE.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_WORKFLOW_004","mode":"reader","query":"How do i find the latest eval scores?","tags":["evaluation","scores","metrics"],"category":"ops_health","gt_answer":"Latest eval scores are stored in metrics/baseline_evaluations/ directory with timestamped versions and configuration metadata. Check metrics/baseline_evaluations/EVALUATION_STATUS.md for current status. Results include ABP files and context metadata sidecars.","expected_files":["metrics/baseline_evaluations/","metrics/baseline_evaluations/EVALUATION_STATUS.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_WORKFLOW_005","mode":"reader","query":"How does the gold star evaluation system work?","tags":["evaluation","system","gold"],"category":"ops_health","gt_answer":"Gold star evaluation system uses three modes: retrieval (tests file finding), reader (tests question answering), and decision (tests decision making). Questions stored in 300_evals/data/gold/v1/gold_cases.jsonl with structured format including id, mode, query, tags, expected_files, gt_answer, and assertions.","expected_files":["300_evals/data/gold/v1/gold_cases.jsonl","400_guides/400_11_performance-optimization.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_MISC_001","mode":"reader","query":"What do i use for listing?","tags":["tools","listing","utilities"],"category":"ops_health","gt_answer":"For file listing and directory operations, use standard Unix commands (ls, find) or Python's pathlib/os modules. The project uses various listing utilities in scripts/ directory for file management and directory traversal.","expected_files":["scripts/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_MISC_002","mode":"reader","query":"Where do you find the rules for my naming conventions?","tags":["naming","conventions","rules"],"category":"ops_health","gt_answer":"Naming conventions are documented in 200_setup/200_naming-conventions.md. Rules include folder prefixes (100_memory, 200_setup, 300_evals, 400_guides, 500_research), file naming patterns, and category prefixes for organized project structure.","expected_files":["200_setup/200_naming-conventions.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_MISC_003","mode":"reader","query":"What are the primary entry points for stateless agents?","tags":["agents","entry","points"],"category":"ops_health","gt_answer":"Primary entry points: 1) 000_core/000_agent-entry-point.md for discovery and execution path, 2) scripts/memory_up.sh for memory rehydration, 3) scripts/unified_memory_orchestrator.py for comprehensive memory orchestration. These provide structured access to project context and capabilities.","expected_files":["000_core/000_agent-entry-point.md","scripts/memory_up.sh","scripts/unified_memory_orchestrator.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_MISC_004","mode":"reader","query":"What do you do if I say to run the memory system?","tags":["memory","system","execution"],"category":"ops_health","gt_answer":"If you say to run the memory system, execute: export POSTGRES_DSN=\"mock://test\" && uv run python scripts/unified_memory_orchestrator.py --systems ltst cursor go_cli prime --role planner \"current project status and core documentation\". This provides comprehensive memory rehydration and context management.","expected_files":["100_memory/100_cursor-memory-context.md","scripts/unified_memory_orchestrator.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_MISC_005","mode":"reader","query":"Where are my gold star eval questions kept?","tags":["evaluation","gold","questions"],"category":"ops_health","gt_answer":"Gold star eval questions are kept in 300_evals/data/gold/v1/gold_cases.jsonl. This file contains 122 evaluation cases covering retrieval, reader, and decision modes with proper JSON formatting and assertions for quality control.","expected_files":["300_evals/data/gold/v1/gold_cases.jsonl"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_MISC_006","mode":"reader","query":"What graphs do i have?","tags":["graphs","visualization","data"],"category":"ops_health","gt_answer":"The project uses various visualization tools including pandas with seaborn/matplotlib for core data visualizations and plotly with dash/altair for interactive visualizations. Graphs are generated for evaluation results, performance metrics, and system monitoring.","expected_files":["scripts/","300_evals/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_001","mode":"reader","query":"What provenance fields are required for every ingested chunk, and which are enforced as NOT NULL?","tags":["provenance","governance","data-lineage"],"category":"ops_health","gt_answer":"Required provenance fields: ingest_run_id (NOT NULL), chunk_variant (NOT NULL), run_id, tool, version, observed_at, confidence. Optional: source_uri, source_path, document_id, meta. Enforced via REQUIRED_META_KEYS in src/memory/guards.py with strict validation. Prevents silent drift and ensures data lineage.","expected_files":["src/memory/models.py","src/memory/guards.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_002","mode":"reader","query":"How do you pin or unset INGEST_RUN_ID during strict evals, and why can pinning cause false failures?","tags":["provenance","evaluation","strict-mode"],"category":"ops_health","gt_answer":"Use EVAL_EXPECT_RUN_ID to pin specific run_id for strict evals. Unset INGEST_RUN_ID and CHUNK_VARIANT for strict gold evaluation: env -u INGEST_RUN_ID -u CHUNK_VARIANT uv run python scripts/ragchecker_official_evaluation.py. Pinning can cause false failures when contexts don't match expected run_id, triggering RuntimeError for run mismatches.","expected_files":["scripts/evaluation/_ragchecker_eval_impl.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_003","mode":"reader","query":"What is the policy for storing text_raw vs text_redacted, and which env flag controls it?","tags":["security","pii","data-storage"],"category":"ops_health","gt_answer":"Text storage policy uses QUERIES_STORE_RAW environment flag. When QUERIES_STORE_RAW=1, raw text is stored; otherwise text is redacted for PII protection. This provides security/PII guardrail and failure-path safety. Configuration managed through pydantic-settings with strict validation.","expected_files":["src/config/settings.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_004","mode":"reader","query":"What is the sessionization rule for queries (e.g., 30-minute reuse), and where is the (session_id, asked_at DESC) index defined?","tags":["sessionization","database","indexing"],"category":"db_workflows","gt_answer":"Sessionization uses 30-minute window for query reuse with (session_id, asked_at DESC) index defined in conv_chunks table. Index created in scripts/utilities/setup_database_schema.py for session-based analytics. Provides UX analytics tied to concrete DB strategy and performance optimization.","expected_files":["scripts/utilities/setup_database_schema.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_005","mode":"reader","query":"What is the fallback policy between exact search and vector search, and under what conditions is each used?","tags":["search","fallback","retrieval"],"category":"rag_qa_single","gt_answer":"Fallback policy: exact search (BM25) used when query is lexically dense, vector search used when query is lexically sparse (cold_start detection). Cold-start boost increases w_vec weight by COLD_START_WVEC_BOOST (default 0.10). Ensures predictable retrieval behavior and latency budgeting.","expected_files":["src/dspy_modules/retriever/pg.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_006","mode":"reader","query":"What is the embedding dimension, normalization policy, and the configured pgvector opclass (cosine vs L2)?","tags":["embeddings","vector","pgvector"],"category":"db_workflows","gt_answer":"Embedding dimension: 1024 (BAAI/bge-large-en-v1.5) or 384 (intfloat/e5-large-v2). Normalization: cosine distance operator (<=>) with 1.0 - (embedding <=> query_vector) scoring. pgvector opclass: cosine distance for similarity search. Prevents silent metric mismatch and guarantees scorer consistency.","expected_files":["templates/eval_manifest_template.yaml","src/dspy_modules/retriever/pg.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_007","mode":"reader","query":"Which ANN index type (IVFFLAT/HNSW) do you use, when do you build it (e.g., post-backfill), and when do you run ANALYZE?","tags":["indexing","ann","performance"],"category":"db_workflows","gt_answer":"Uses HNSW index type for pgvector with cosine distance. Index built post-backfill during database setup. ANALYZE run nightly via scripts for performance optimization. Keeps recall/latency stable across ingest cycles. Configuration in database schema setup scripts.","expected_files":["scripts/utilities/setup_database_schema.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_008","mode":"reader","query":"What tokenizer is used across chunking and embedding, and how do you enforce tokenizer/embedding compatibility?","tags":["tokenization","chunking","compatibility"],"category":"db_workflows","gt_answer":"Tokenizer: SentenceTransformer tokenizer (BAAI/bge-large-en-v1.5 or intfloat/e5-large-v2). Chunking uses 450 tokens with 0.10 overlap ratio. Compatibility enforced through consistent tokenizer usage across chunking and embedding stages. Eliminates off-by-token fragmentation errors that nuke recall.","expected_files":["templates/eval_manifest_template.yaml","300_evals/configs/yaml/base.yaml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_009","mode":"reader","query":"What chunk size and overlap profiles are used per content type (guides, code, transcripts), and where are they configured?","tags":["chunking","profiles","configuration"],"category":"db_workflows","gt_answer":"Default chunk size: 450 tokens, overlap ratio: 0.10, Jaccard threshold: 0.8 for all content types. Configured in templates/eval_manifest_template.yaml and 300_evals/configs/yaml/base.yaml. Makes chunking predictable and auditable (not library defaults).","expected_files":["templates/eval_manifest_template.yaml","300_evals/configs/yaml/base.yaml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_010","mode":"reader","query":"What happens when embedding or tokenization fails, and how are such rows flagged (e.g., meta.error='embed_failed', embedding=NULL)?","tags":["error-handling","failure-modes","data-quality"],"category":"ops_health","gt_answer":"Failed embeddings/tokenization are flagged with meta.error='embed_failed' and embedding=NULL. Error handling implemented in data processing scripts with proper validation. Prevents corrupt rows from polluting evals. Error tracking through metadata fields and structured logging.","expected_files":["scripts/data_processing/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_011","mode":"reader","query":"What telemetry is recorded per query (e.g., latency_ms, model, reranker_on), and where do you view it (Logfire spans)?","tags":["telemetry","monitoring","observability"],"category":"ops_health","gt_answer":"Telemetry recorded: latency_ms, model_id, reranker_enabled, retrieval_candidates, retrieved_context, variant_policy. Viewed through Logfire spans in 300_evals/evals/ with structured logging. Enables regression diagnosis without spelunking.","expected_files":["300_evals/evals/qa_eval.py","300_evals/evals/retrieval_eval.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_012","mode":"reader","query":"Which environment variables are required/optional to boot the system, and where are defaults validated (pydantic-settings)?","tags":["configuration","environment","validation"],"category":"ops_health","gt_answer":"Required: POSTGRES_DSN, EVAL_PROFILE. Optional: APP_ENV, APP_ROOT_DIR, APP_STRICT_PROVENANCE, APP_LIMIT_CONCURRENCY. Defaults validated in src/config/settings.py using pydantic-settings with clear precedence: init_settings → env_settings → dotenv → YAML → file_secrets. Hardens startup and documents contract.","expected_files":["src/config/settings.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_013","mode":"reader","query":"What are the two venvs' activation commands and intended use cases, and how does CI pick the right one?","tags":["venv","environments","ci"],"category":"ops_health","gt_answer":"Two venvs: .venv (local macOS, UV_PROJECT_ENVIRONMENT=.venv) and /opt/venv (Docker/CI Linux, UV_PROJECT_ENVIRONMENT=/opt/venv). CI picks via astral-sh/setup-uv@v4 and uv sync --frozen. Prevents 'works on my machine' failures through explicit environment selection.","expected_files":["scripts/unified_memory_orchestrator.py",".github/workflows/eval.yml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_014","mode":"reader","query":"What is the single-command 'smoke test' to validate LLM wiring outside the eval harness, and what output confirms success?","tags":["testing","smoke-test","validation"],"category":"ops_health","gt_answer":"Smoke test: curl http://localhost:1234/v1/chat/completions -H 'Content-Type: application/json' -d '{\"model\":\"Yi-Coder-9B-Chat-Q6_K\",\"messages\":[{\"role\":\"user\",\"content\":\"print(2+2)\"}],\"temperature\":0.2}'. Success confirmed by JSON response containing '4'. Cuts through harness indirection to verify pipe generates text.","expected_files":["200_setup/201_model-configuration.md"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_ADVANCED_015","mode":"reader","query":"Which MCP servers (if any) are configured, what capabilities do they expose, and how do agents authenticate to them?","tags":["mcp","servers","authentication"],"category":"ops_health","gt_answer":"MCP server configured on localhost:3000 with capabilities: get_project_context, run_precision_eval, query_memory. Authentication via HTTP API calls. Agents access through MCP client integration. Makes tool access explicit and safe for stateless agents.","expected_files":["scripts/utilities/mcp_memory_server.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_001","mode":"reader","query":"What is the grading rubric for answers (exact-match, regex, semantic), and what normalization is applied (case/whitespace/punctuation)?","tags":["grading","rubric","normalization"],"category":"ops_health","gt_answer":"Grading uses normalized matching with case-insensitive, whitespace-normalized comparison. Normalization: strip trailing semicolons/backticks, normalize whitespace to single spaces, enforce 180 char limit. Tag-specific normalization for db_workflows (SQL line extraction). Prevents false fails from formatting differences.","expected_files":["src/dspy_modules/reader/span_picker.py","src/dspy_modules/reader/program.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_002","mode":"reader","query":"How are 'unanswerable' cases represented and scored, and what string(s) are accepted by the harness?","tags":["unanswerable","abstention","scoring"],"category":"ops_health","gt_answer":"Unanswerable cases use 'NOT_ANSWERABLE' string (case-insensitive). Special handling for 'Not in context.' triggers allow_abstain=true and must_abstain=true assertions. Scored as correct when system abstains appropriately. Prevents NOT_ANSWERABLE trap by accepting multiple representations.","expected_files":["src/dspy_modules/reader/program.py","scripts/evaluation/annotate_gold_assertions.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_003","mode":"reader","query":"What are the pass/fail thresholds per metric (Recall@k, MRR, nDCG, accuracy) and per stage (retrieval, rerank, generation)?","tags":["thresholds","metrics","gates"],"category":"ops_health","gt_answer":"Baseline thresholds: Recall@20 ≥0.65-0.75, Precision@k ≥0.20-0.35, Faithfulness ≥0.60-0.75, Unsupported Claims ≤10-15%, Context Utilization ≥60%. Reranker lift +10-20%. P95 latency ≤4s. Hard gates block development until targets met. Defines gates clearly for each stage.","expected_files":["scripts/evaluation/metrics_guard.py","scripts/evaluation/baseline_version_manager.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_004","mode":"reader","query":"How is statistical significance computed across runs (replicates, fixed seeds, bootstrap CIs), and what delta triggers a regression alert?","tags":["statistics","significance","regression"],"category":"ops_health","gt_answer":"Uses fixed seeds (SEED=42) for reproducibility. Statistical significance via bootstrap CIs with configurable delta thresholds. Regression alerts trigger on precision drift >2%, latency increase >15%, F1 score below baseline. Guards against noise with deterministic evaluation.","expected_files":["scripts/evaluation/baseline_metrics_collector.py","scripts/monitoring/kpi_monitor.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_005","mode":"reader","query":"What is the canonical baseline (dataset version, model build, run_id) used for regression comparison, and where is it pinned?","tags":["baseline","regression","versioning"],"category":"ops_health","gt_answer":"Canonical baseline pinned in metrics/baseline_evaluations/ with specific run_id, dataset version, and model build. Baseline comparison uses locked configuration from configs/stable_bedrock.env. Prevents baseline drift by maintaining stable reference point for regression detection.","expected_files":["scripts/evaluation/baseline_version_manager.py","scripts/shell/deployment/run_locked_eval.sh"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_006","mode":"reader","query":"How are eval datasets versioned and diffed, and where is their change log maintained?","tags":["versioning","datasets","changelog"],"category":"ops_health","gt_answer":"Datasets versioned with timestamps and config hashes. Change log maintained in metrics/baseline_evaluations/ with evaluation_run.json artifacts. Diffing via git tracking and artifact comparison. Makes ground truth auditable through version control and metadata tracking.","expected_files":["scripts/evaluation/_ragchecker_eval_impl.py","metrics/baseline_evaluations/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_007","mode":"reader","query":"What anti-leakage policy prevents training/eval contamination (holdouts, time-based splits), and how is it enforced?","tags":["anti-leakage","contamination","validation"],"category":"ops_health","gt_answer":"Anti-leakage enforced through strict provenance tracking with ingest_run_id and chunk_variant validation. Time-based splits and holdout sets prevent contamination. EVAL_STRICT_VARIANTS=1 enforces strict validation. Preserves validity through data lineage tracking.","expected_files":["src/memory/guards.py","scripts/evaluation/_ragchecker_eval_impl.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_008","mode":"reader","query":"What is the proposal→review→approval workflow for adding/changing gold questions (PR template, required reviewers)?","tags":["workflow","governance","process"],"category":"ops_health","gt_answer":"Gold questions follow PR-based workflow with structured review process. Changes require PR with proper documentation and testing. Review process includes validation of assertions and file references. Process ensures quality control and prevents bad test cases.","expected_files":["300_evals/data/gold/v1/gold_cases.jsonl"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_009","mode":"reader","query":"How is category coverage tracked (docs/code/transcripts, difficulty tiers, topic distribution), and what are the minimum quotas?","tags":["coverage","categories","quotas"],"category":"ops_health","gt_answer":"Category coverage tracked through tags system: ops_health, db_workflows, rag_qa_single, rag_qa_multi, meta_ops, negatives. Balanced distribution across content types (docs, code, transcripts). Minimum quotas ensure comprehensive test coverage across all system components.","expected_files":["300_evals/data/gold/v1/gold_cases.jsonl"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_010","mode":"reader","query":"What is the policy for identifying and handling flaky evals (triage window, quarantine rules, re-enable criteria)?","tags":["flaky","triage","quarantine"],"category":"ops_health","gt_answer":"Flaky evals identified through statistical analysis and monitoring. Quarantine rules isolate problematic tests. Re-enable criteria based on stability metrics and root cause analysis. Reduces alert fatigue through systematic flaky test management.","expected_files":["scripts/monitoring/kpi_monitor.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_011","mode":"reader","query":"How do you isolate stages for diagnosis (retrieval-only, reranker-only, generation-only switches), and what flags control each?","tags":["isolation","diagnosis","debugging"],"category":"ops_health","gt_answer":"Stage isolation via environment flags: RERANK_ENABLE=0/1 for reranker, retrieval-only mode for retrieval testing, generation-only for answer quality. Targeted debugging through modular evaluation components. Enables precise diagnosis of system failures.","expected_files":["scripts/evaluation/ci_gate_reader.py","src/dspy_modules/retriever/pg.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_012","mode":"reader","query":"What configuration matrix is exercised routinely (models × reranker on/off × k values × chunk profiles), and which subsets run on PR vs nightly?","tags":["configuration","matrix","testing"],"category":"ops_health","gt_answer":"Configuration matrix: models (Mistral 7B, Yi-Coder), reranker (on/off), k values (25, 40, 60), chunk profiles (450 tokens, 0.10 overlap). PR runs: smoke tests with minimal config. Nightly runs: full matrix with all combinations. Smart test budgeting for efficient coverage.","expected_files":[".github/workflows/eval.yml","scripts/shell/deployment/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_013","mode":"reader","query":"Which determinism settings are required (seed, temperature, top_p, Python hash seed), and how do they differ for local vs cloud LLMs?","tags":["determinism","reproducibility","settings"],"category":"ops_health","gt_answer":"Determinism settings: SEED=42, temperature=0 for evals, top_p=0.9, Python hash seed fixed. Local LLMs: full determinism control. Cloud LLMs: limited determinism, rely on seed and temperature. Ensures reproducible evaluation results across environments.","expected_files":["scripts/evaluation/_ragchecker_eval_impl.py","templates/eval_manifest_template.yaml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_014","mode":"reader","query":"Which caches must be disabled during evals (LLM cache, retrieval cache, HTTP), and how are they toggled off?","tags":["caching","evals","isolation"],"category":"ops_health","gt_answer":"Caches disabled during evals: LLM cache (temperature=0), retrieval cache (fresh queries), HTTP cache (no-cache headers). Toggled via environment variables and eval configuration. Prevents ghost wins from cached results.","expected_files":["templates/eval_manifest_template.yaml","scripts/evaluation/_ragchecker_eval_impl.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_015","mode":"reader","query":"Where are per-run artifacts stored (scores.json, per-question traces, span IDs), and what is the retention policy?","tags":["artifacts","storage","retention"],"category":"ops_health","gt_answer":"Per-run artifacts stored in metrics/baseline_evaluations/ with timestamped directories. Includes scores.json, evaluation_run.json, per-question traces, span IDs. Retention policy: keep latest 10 runs, archive older runs. Provides forensic trail for debugging.","expected_files":["scripts/evaluation/_ragchecker_eval_impl.py","metrics/baseline_evaluations/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_016","mode":"reader","query":"How are failures auto-labeled (dataset error vs retrieval fault vs rerank fault vs generator fault), and where is that label persisted?","tags":["failure-labeling","diagnosis","persistence"],"category":"ops_health","gt_answer":"Failures auto-labeled through error analysis: dataset errors (missing files), retrieval faults (low recall), rerank faults (poor ranking), generator faults (hallucination). Labels persisted in evaluation artifacts and monitoring systems. Speeds triage through automated classification.","expected_files":["scripts/monitoring/kpi_monitor.py","scripts/evaluation/baseline_metrics_collector.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_017","mode":"reader","query":"What tooling exists to inspect a failing item (trace link, query → chunks → answer view), and how do you open it from a run ID?","tags":["debugging","tooling","inspection"],"category":"ops_health","gt_answer":"Debugging tooling: trace links in evaluation artifacts, query→chunks→answer view in monitoring dashboards. Access via run ID through metrics/baseline_evaluations/ directory. First-class debuggability through structured logging and artifact organization.","expected_files":["metrics/baseline_evaluations/","scripts/monitoring/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_018","mode":"reader","query":"What is the cost/time budget for full and smoke suites, and what levers (k, sample size, concurrency) you tune to stay within it?","tags":["budget","performance","optimization"],"category":"ops_health","gt_answer":"Cost/time budget: smoke suite <5 minutes, full suite <30 minutes. Levers: k values (25→40→60), sample size (subset for PR), concurrency (MAX_WORKERS=3). Tuning via environment variables and configuration. Keeps suite fast and affordable.","expected_files":["scripts/shell/deployment/","configs/profiles/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_019","mode":"reader","query":"What concurrency limits are used for eval execution (e.g., ThreadPoolExecutor 2–3 workers / optional asyncio), and why that level?","tags":["concurrency","performance","stability"],"category":"ops_health","gt_answer":"Concurrency limits: MAX_WORKERS=3 for local, 8 for CI. ThreadPoolExecutor with controlled parallelism. Avoids resource thrash and flakiness while maintaining reasonable speed. Balanced approach for stability vs performance.","expected_files":["scripts/evaluation/_ragchecker_eval_impl.py","configs/profiles/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_020","mode":"reader","query":"How is environment parity enforced across local/CI/prod (Python version, lockfiles, model binaries), and how do we detect drift?","tags":["environment","parity","detection"],"category":"ops_health","gt_answer":"Environment parity via uv.lock files, Docker containers, and version pinning. Python 3.12 enforced across all environments. Drift detection through CI checks and environment validation. 'Works everywhere' guarantee through consistent tooling.","expected_files":[".github/workflows/eval.yml","pyproject.toml","uv.lock"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_021","mode":"reader","query":"What schedules trigger which suites (PR, nightly, weekly full), and what branch patterns or tags control them?","tags":["scheduling","triggers","automation"],"category":"ops_health","gt_answer":"Schedules: PR triggers smoke tests, nightly runs full evaluation, weekly runs comprehensive suite. Branch patterns: main branch for nightly, PR branches for smoke tests. Predictable cadence through GitHub Actions workflows.","expected_files":[".github/workflows/eval.yml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_022","mode":"reader","query":"What is the escape hatch to quarantine a broken eval set or flaky group without blocking merges (feature flag, skip list, marker)?","tags":["quarantine","escape-hatch","safety"],"category":"ops_health","gt_answer":"Escape hatch: RAGCHECKER_DISABLE_SAFE_GUARD=1 to bypass safety checks, skip markers for flaky tests, feature flags for problematic components. Safety valve prevents broken evals from blocking development.","expected_files":["scripts/shell/deployment/run_locked_eval.sh","scripts/evaluation/_ragchecker_eval_impl.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_023","mode":"reader","query":"How do we validate 'must_include' anchors for retrieval golds (precheck script, CI step) to ensure the doc path actually exists?","tags":["validation","anchors","precheck"],"category":"ops_health","gt_answer":"Validation via precheck scripts that verify expected_files exist before evaluation. CI step validates file paths and anchors. Prevents bad tests by ensuring all referenced files are present and accessible.","expected_files":["scripts/evaluation/",".github/workflows/eval.yml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_024","mode":"reader","query":"What is the policy for updating gold answers when source docs change (pin to doc version vs rolling), and who signs off?","tags":["gold-updates","versioning","governance"],"category":"ops_health","gt_answer":"Gold answer updates: rolling updates when source docs change, with proper review process. Sign-off required from maintainers. Stable targets through version control and change tracking. Prevents stale gold answers.","expected_files":["300_evals/data/gold/v1/gold_cases.jsonl"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_PROCESS_025","mode":"reader","query":"How are results communicated (commit status checks, PR comment summary, Slack/Ankyr notifications), and what fields are included in the summary?","tags":["communication","notifications","feedback"],"category":"ops_health","gt_answer":"Results communicated via commit status checks, PR comment summaries, and monitoring dashboards. Summary includes: precision, recall, F1 score, latency, pass/fail status. Tight feedback loop through automated reporting and notifications.","expected_files":[".github/workflows/eval.yml","scripts/monitoring/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_001","mode":"reader","query":"What retrieval stack and fusion algorithm do we use (e.g., BM25 + pgvector), what are the top_k values at each stage, and where is this configured?","tags":["retrieval","fusion","configuration"],"category":"ops_health","gt_answer":"Retrieval stack: BM25 + pgvector with weighted RRF fusion (lambda_lex=0.6, lambda_sem=0.4). Top-k values: stage1_top_k=50, stage2_top_k=8, shortlist=25, topk=25. Configured in configs/retriever_weights.yaml and environment variables. Uses hybrid retrieval with semantic and lexical components.","expected_files":["src/retrieval/fusion.py","scripts/evaluation/eval_manifest_generator.py","src/config/models.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_002","mode":"reader","query":"How are queries normalized/reformulated before retrieval (history condensation, stopword/symbol cleanup, synonym expansion), and which module performs it?","tags":["query-normalization","reformulation","preprocessing"],"category":"ops_health","gt_answer":"Query normalization via build_channel_queries() in src/dspy_modules/retriever/query_rewrite.py. Includes: tag-specific hints, phrase hints, stopword cleanup, symbol normalization. No history condensation or synonym expansion. Channel-specific query building for short/title/BM25/vector channels.","expected_files":["src/dspy_modules/retriever/query_rewrite.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_003","mode":"reader","query":"What rules govern when conversation history is included vs ignored for retrieval, and what is the max token budget for history?","tags":["conversation-history","token-budget","retrieval-rules"],"category":"ops_health","gt_answer":"Conversation history handled by LTST memory system with max_context_length=10000, history_limit=20. No explicit token budget for retrieval. History inclusion based on relevance_threshold=0.7, similarity_threshold=0.8. Memory rehydration determines context inclusion.","expected_files":["scripts/unified_memory_orchestrator.py","src/utils/memory_rehydrator.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_004","mode":"reader","query":"What freshness filters or time-decay boosts are applied, and how are published_at/updated_at used in scoring or filtering?","tags":["freshness","time-decay","scoring"],"category":"ops_health","gt_answer":"No explicit freshness filters or time-decay boosts implemented. No published_at/updated_at fields used in scoring. Scoring based on BM25 + vector similarity + reranking scores. Time-based filtering not currently implemented in retrieval pipeline.","expected_files":["src/retrieval/fusion.py","src/dspy_modules/retriever/pg.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_005","mode":"reader","query":"How do we enforce diversity in the retrieved set (e.g., MMR or source-balancing), and what parameters control it?","tags":["diversity","mmr","source-balancing"],"category":"ops_health","gt_answer":"Diversity enforced via MMR reranking with alpha=0.85, per_file_penalty=0.10, per_file_cap=5. MMR applied when cross-encoder reranking disabled. Source balancing through per_file_cap limiting chunks per file. MMR parameters configurable via MMR_ALPHA, PER_FILE_CAP environment variables.","expected_files":["src/dspy_modules/dspy_reader_program.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_006","mode":"reader","query":"How are per-source weights applied (guides vs code vs transcripts), and where are those weights defined and tested?","tags":["source-weights","scoring","configuration"],"category":"ops_health","gt_answer":"Per-source weights defined in configs/retriever_weights.yaml with tag-specific overrides. Weights applied during fusion via weighted_rrf() function. Tag-specific configurations for ops_health, db_workflows, meta_ops, rag_qa_single. Weights tested through evaluation harness and configuration validation.","expected_files":["configs/retriever_weights.yaml","src/retrieval/fusion.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_007","mode":"reader","query":"What reranker is used post-retrieval, what inputs/metadata does it see, and what threshold trims candidates?","tags":["reranker","cross-encoder","thresholds"],"category":"ops_health","gt_answer":"Cross-encoder reranker: cross-encoder/ms-marco-MiniLM-L-6-v2. Inputs: query + document text. Thresholds: input_topk=50, keep=12. Metadata: chunk content, provenance. Fallback to heuristic reranking if cross-encoder fails. Configurable via RERANK_ENABLE, RERANK_INPUT_TOPK, RERANK_KEEP.","expected_files":["src/dspy_modules/retriever/reranker_config.py","src/rag/reranker_env.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_008","mode":"reader","query":"What is the max context budget for injected evidence, how are chunks ordered, and what policy evicts overflow (by score, novelty, or recency)?","tags":["context-budget","chunk-ordering","eviction"],"category":"ops_health","gt_answer":"Max context budget: CONTEXT_MAX_CHARS=1600, CONTEXT_DOCS_MAX=12. Chunks ordered by reranking scores (cross-encoder or MMR). Eviction policy: per_file_cap=5 limits chunks per file, then topk=25 final limit. No explicit overflow eviction by score/novelty/recency.","expected_files":["scripts/evaluation/eval_manifest_generator.py","src/dspy_modules/dspy_reader_program.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_009","mode":"reader","query":"What canonicalization/de-duplication runs on chunks (Markdown/HTML stripping, code-fence normalization, semantic dedupe), and how is 'near-duplicate' defined?","tags":["canonicalization","deduplication","normalization"],"category":"ops_health","gt_answer":"Canonicalization: Markdown stripping, code-fence normalization via _normalize_answer(). De-duplication via jaccard_threshold=0.8 for semantic similarity. Near-duplicate defined as Jaccard similarity >0.8 between chunk content. No HTML stripping implemented. Deduplication configurable via jaccard_threshold parameter.","expected_files":["src/dspy_modules/reader/span_picker.py","src/config/models.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_010","mode":"reader","query":"What guardrails mitigate prompt-injection from retrieved content (quote fencing, deny-lists, pattern checks), and how are violations logged and surfaced?","tags":["guardrails","prompt-injection","security"],"category":"ops_health","gt_answer":"No explicit prompt-injection guardrails implemented. No quote fencing, deny-lists, or pattern checks for retrieved content. Security relies on input validation and output filtering. No violation logging or alerting system for prompt injection attempts.","expected_files":["src/dspy_modules/dspy_reader_program.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_011","mode":"reader","query":"What is the citation/attribution policy—required fields per citation, linkback to source URIs, and per-sentence vs per-answer granularity?","tags":["citation","attribution","policy"],"category":"ops_health","gt_answer":"Citation policy: per-answer granularity with source URIs. Required fields: source_uri, chunk_id, document_id. Linkback to source URIs via expected_files in gold cases. No per-sentence citation granularity. Attribution handled through provenance tracking and source metadata.","expected_files":["src/schemas/eval.py","300_evals/data/gold/v1/gold_cases.jsonl"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_012","mode":"reader","query":"What happens when retrieval returns fewer than N acceptable chunks—minimum viable context rules, fallback retriever, or emit 'unanswerable'?","tags":["fallback","minimum-context","unanswerable"],"category":"ops_health","gt_answer":"When retrieval returns fewer chunks: no explicit minimum viable context rules. Fallback to 'NOT_ANSWERABLE' if insufficient context. No fallback retriever implemented. System relies on reranking and MMR to ensure quality chunks. Unanswerable detection via token overlap <50% with context.","expected_files":["src/dspy_modules/reader/program.py","src/dspy_modules/dspy_reader_program.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_013","mode":"reader","query":"How are multilingual or code-heavy queries detected, and how does that change tokenizer, retriever, or reranker choices?","tags":["multilingual","code-detection","tokenizer"],"category":"ops_health","gt_answer":"No explicit multilingual or code-heavy query detection implemented. No tokenizer switching based on query type. Single tokenizer (BAAI/bge-small-en-v1.5) used for all queries. No language-specific retrieval or reranking strategies.","expected_files":["src/dspy_modules/retriever/query_rewrite.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_014","mode":"reader","query":"What structure-aware chunking rules are used (headings/paragraphs/code blocks), and how are size/overlap tuned per document type?","tags":["chunking","structure-aware","document-types"],"category":"ops_health","gt_answer":"Chunking rules: chunk_size=450, overlap_ratio=0.10, jaccard_threshold=0.8. No explicit structure-aware chunking for headings/paragraphs/code blocks. No document-type-specific tuning. Single chunking strategy applied to all content types.","expected_files":["src/config/models.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_RAG_015","mode":"reader","query":"How do we trace a single RAG answer end-to-end (query → rewrite → retrieve → rerank → context pack → answer), and what command/URL opens that trace for a given run_id?","tags":["tracing","end-to-end","debugging"],"category":"ops_health","gt_answer":"End-to-end tracing via evaluation artifacts in metrics/baseline_evaluations/ with run_id directories. Trace includes: query, rewritten queries, retrieved chunks, reranking scores, context pack, final answer. Access via run_id through evaluation artifact files. No web URL for trace viewing.","expected_files":["scripts/evaluation/_ragchecker_eval_impl.py","metrics/baseline_evaluations/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_001","mode":"reader","query":"What is the single source of truth for dependencies and how are prod/dev/test extras separated (and locked)?","tags":["dependencies","extras","locking"],"category":"ops_health","gt_answer":"pyproject.toml is single source of truth for dependencies. Extras: test=['spacy>=3.7.0'], dev=[pytest, black, ruff, pre-commit, mypy, etc.], security=[bandit, safety, pip-audit], ml=[dspy, torch, sentence-transformers]. Locked via uv.lock file with platform-specific markers. Prevents drift and 'it works on my machine' issues.","expected_files":["pyproject.toml","uv.lock"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_002","mode":"reader","query":"What Python versions are officially supported/enforced across local and CI, and how is drift detected?","tags":["python-version","enforcement","drift-detection"],"category":"ops_health","gt_answer":"Python 3.12 officially supported and enforced. CI uses actions/setup-python@v5 with python-version: '3.12'. pyproject.toml requires-python: '>=3.12'. Drift detected via CI checks and basedpyright configuration. Ensures reproducibility and fewer mystery type errors.","expected_files":[".github/workflows/","pyproject.toml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_003","mode":"reader","query":"Do we use the src/ layout, and what import path rules and __init__.py conventions are enforced?","tags":["src-layout","imports","conventions"],"category":"ops_health","gt_answer":"Yes, uses src/ layout. pytest.ini sets pythonpath=['src'] for proper module resolution. Import rules: absolute imports, no sys.path hacks. __init__.py files required in packages. basedpyright includes src/ in extraPaths. Ensures import hygiene and tooling consistency.","expected_files":["pytest.ini","src/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_004","mode":"reader","query":"Which pre-commit hooks are mandatory (ruff/black/pyupgrade/codespell/etc.), and are they mirrored in CI?","tags":["pre-commit","hooks","ci-mirroring"],"category":"ops_health","gt_answer":"Mandatory pre-commit hooks: ruff (--fix, --exit-non-zero-on-fix), ruff-format, mypy (--strict, --show-error-codes). Mirrored in CI via .github/workflows/ci-ruff-pyupgrade.yml and .github/workflows/dry-run.yml. Keeps style and fixes automated across local and CI.","expected_files":[".pre-commit-config.yaml",".github/workflows/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_005","mode":"reader","query":"What Ruff config do we use (rule sets, target-version, ignores) and why those choices?","tags":["ruff","configuration","rules"],"category":"ops_health","gt_answer":"Ruff config: target-version='py312', line-length=120, select=['E','F','I','UP'] (pyupgrade enabled), ignores=['E501','E402','E722','F401','F841','F821','F402','E741']. Excludes venv, 600_archives, docs/legacy, __pycache__. Makes lint trade-offs explicit and consistent.","expected_files":["pyproject.toml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_006","mode":"reader","query":"Which type checker is canonical (pyright vs mypy), what strictness level, and how are # type: ignore tracked?","tags":["type-checker","strictness","type-ignore"],"category":"ops_health","gt_answer":"basedpyright is canonical type checker (replaces pyright). Strictness: reportMissingTypeStubs='none', reportUnknownMemberType='none', reportExplicitAny='none'. # type: ignore tracked via basedpyright configuration. Consistent, audit-able typing across codebase.","expected_files":["pyproject.toml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_007","mode":"reader","query":"What pytest markers exist (e.g., slow, db, e2e, gpu) and what is the default skip/selection policy in CI vs local?","tags":["pytest","markers","selection-policy"],"category":"ops_health","gt_answer":"Pytest markers: unit, integration, e2e, smoke, tier1/2/3, slow, fast, deprecated, asyncio, legacy, critical, flaky, prop. CI: runs critical markers on PR, full suite on main. Local: --import-mode=importlib, excludes 600_archives, 300_experiments. Faster feedback, less flakiness through marker-based selection.","expected_files":["pytest.ini",".github/workflows/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_008","mode":"reader","query":"What coverage thresholds (line/branch) are enforced and where is .coveragerc?","tags":["coverage","thresholds","configuration"],"category":"ops_health","gt_answer":"Coverage config in .coveragerc: branch=True, parallel=True, concurrency=thread. Omits */site-packages/*, tests/*. No explicit thresholds enforced in CI. Guards against silent test rot through coverage tracking.","expected_files":[".coveragerc"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_009","mode":"reader","query":"How do we use Hypothesis (profiles, deadlines, health checks) and what rules mitigate flakiness?","tags":["hypothesis","property-tests","flakiness"],"category":"ops_health","gt_answer":"Hypothesis used for property-based testing with pytest marker 'prop'. Default pytest config disables hypothesis (-p no:hypothesis). Property tests run nightly only. No explicit profiles, deadlines, or health checks configured. Mitigates flakiness through nightly-only execution.","expected_files":["pytest.ini","pyproject.toml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_010","mode":"reader","query":"Do we use snapshot testing; where are snapshots stored; and how are diffs reviewed and approved?","tags":["snapshot-testing","storage","review"],"category":"ops_health","gt_answer":"No snapshot testing implemented. No snapshots stored. No diff review/approval process for snapshots. Stable API/format guarantees not enforced through snapshot testing.","expected_files":[],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_011","mode":"reader","query":"How is the test database provisioned (Docker service, testcontainers), seeded, and torn down—migrations or fixtures first?","tags":["test-database","provisioning","migrations"],"category":"ops_health","gt_answer":"Test database provisioned via PostgreSQL service in CI. No testcontainers used. Database seeded through fixtures and migrations. Clean, realistic DB tests through proper setup/teardown.","expected_files":[".github/workflows/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_012","mode":"reader","query":"What is our Alembic policy (deterministic migrations, no IF NOT EXISTS, review checklist) and how are they generated?","tags":["alembic","migrations","policy"],"category":"ops_health","gt_answer":"No Alembic policy documented. No explicit migration generation process. No IF NOT EXISTS restrictions. No review checklist for migrations. Safe schema evolution not enforced through migration policies.","expected_files":[],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_013","mode":"reader","query":"What is the data retention/archival policy for logs/artifacts (eval traces, spans), and how do we VACUUM/GC?","tags":["retention","archival","cleanup"],"category":"ops_health","gt_answer":"Data retention: evaluation artifacts stored in metrics/baseline_evaluations/ with timestamped directories. Log rotation: RotatingFileHandler with maxBytes=50MB, backupCount=5. No explicit VACUUM/GC policy documented. Keeps storage/costs sane through log rotation.","expected_files":["src/utils/logger.py","metrics/baseline_evaluations/"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_014","mode":"reader","query":"How are secrets loaded locally vs CI (pydantic-settings, Actions secrets), and how do we prevent accidental commits (secret scanning)?","tags":["secrets","loading","security"],"category":"ops_health","gt_answer":"Secrets loaded via pydantic-settings with SecretStr fields. CI uses GitHub Actions secrets. No explicit secret scanning in CI. Security relies on pydantic-settings validation and environment variable management.","expected_files":["src/config/models.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_015","mode":"reader","query":"What structured logging format do we use (JSON), which fields are mandatory (run_id, session_id, span_id), and where do logs ship (Logfire)?","tags":["logging","format","observability"],"category":"ops_health","gt_answer":"Structured logging: JSON format via StructuredFormatter. Mandatory fields: timestamp, level, logger, message, module, function, line. Optional: request_id, session_id, span_id. Logs ship to Logfire via logfire.configure() if LOGFIRE_API_KEY present. Turn bugs into traces, not guesswork.","expected_files":["src/utils/logger.py","src/observability/logging.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_016","mode":"reader","query":"What is our exception taxonomy (domain base classes), and what are the fail-fast vs retry rules?","tags":["exceptions","taxonomy","retry-rules"],"category":"ops_health","gt_answer":"No explicit exception taxonomy documented. No domain base classes for exceptions. No fail-fast vs retry rules documented. Predictable error handling not enforced through exception hierarchy.","expected_files":[],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_017","mode":"reader","query":"What's the canonical dev workflow (Cursor agent preamble/.cursor config, Make/Just/uv tasks), and the top 10 commands every dev should know?","tags":["dev-workflow","commands","onboarding"],"category":"ops_health","gt_answer":"Canonical dev workflow: uv for dependency management, pytest for testing, ruff for linting. Top commands: uv sync, uv run pytest, uv run ruff check, uv run pyright. No Make/Just tasks documented. No .cursor config documented. Lowers onboarding friction through uv-based workflow.","expected_files":["pyproject.toml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_018","mode":"reader","query":"Where are n8n workflows stored/versioned, how do they interact with the repo, and how are they tested?","tags":["n8n","workflows","versioning"],"category":"ops_health","gt_answer":"No n8n workflows documented. No workflow storage/versioning process. No interaction with repo. No testing of workflows. Orchestration as code not implemented.","expected_files":[],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_019","mode":"reader","query":"What caches exist (HTTP, LLM, embeddings, retrieval), how are keys/TTLs defined, and how are caches disabled for tests?","tags":["caching","keys","ttl"],"category":"ops_health","gt_answer":"Caches: LLM cache (temperature=0), retrieval cache (fresh queries), HTTP cache (no-cache headers). No explicit key/TTL definitions documented. Caches disabled via environment variables during evals. Avoids ghost passes/fails through cache isolation.","expected_files":["templates/eval_manifest_template.yaml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_020","mode":"reader","query":"What profiling/benchmark tools are used (cProfile/pyinstrument), what baseline benchmarks exist, and where are regressions tracked?","tags":["profiling","benchmarks","regressions"],"category":"ops_health","gt_answer":"Profiling tools: memory-profiler, py-spy available in dev extras. No explicit cProfile/pyinstrument usage documented. No baseline benchmarks documented. No regression tracking system. Performance measurement not systematically implemented.","expected_files":["pyproject.toml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_021","mode":"reader","query":"Where do we intentionally use limited concurrency (ThreadPoolExecutor 2–3 workers / optional asyncio), and what guards prevent DB/LLM saturation?","tags":["concurrency","threading","guards"],"category":"ops_health","gt_answer":"Limited concurrency: MAX_WORKERS=3 for local, 8 for CI. ThreadPoolExecutor with controlled parallelism. No explicit asyncio usage documented. Guards: rate limiting via BEDROCK_MAX_RPS=0.12, timeout_sec=35. Stable throughput without thrash through concurrency limits.","expected_files":["scripts/evaluation/eval_manifest_generator.py"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_022","mode":"reader","query":"Which security checks run in CI (pip-audit/safety, Bandit/Ruff-B, license scanning), and what's the CVE triage SLA?","tags":["security","ci-checks","cve-triage"],"category":"ops_health","gt_answer":"Security checks: bandit, safety, pip-audit in security extras. No explicit CI security checks documented. No CVE triage SLA documented. Supply chain security not systematically enforced in CI.","expected_files":["pyproject.toml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_023","mode":"reader","query":"What versioning scheme (SemVer/CalVer), tagging rules, and changelog automation drive releases—and what triggers one?","tags":["versioning","tagging","releases"],"category":"ops_health","gt_answer":"Versioning: pyproject.toml version='0.1.0'. Commitizen configured for conventional commits. No explicit SemVer/CalVer scheme documented. No tagging rules documented. No changelog automation documented. No release triggers documented.","expected_files":["pyproject.toml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_024","mode":"reader","query":"How are docs built (MkDocs/Sphinx), where are ADRs kept, and how do we Jinja2-sync code examples with docs?","tags":["documentation","building","sync"],"category":"ops_health","gt_answer":"No documentation building system documented. No MkDocs/Sphinx configuration. No ADR storage documented. No Jinja2-sync for code examples documented. Living docs not implemented through automated building/sync.","expected_files":[],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
{"id":"GOLD_REPO_025","mode":"reader","query":"What git hygiene rules apply (Conventional Commits, branch naming, PR checklist), and what is the backport or hotfix policy?","tags":["git-hygiene","commits","branching"],"category":"ops_health","gt_answer":"Git hygiene: Conventional Commits via commitizen (feat, fix, refactor, docs, chore, ci, test, build, bump, perf, revert, style, merge). No branch naming rules documented. No PR checklist documented. No backport/hotfix policy documented. Clean history through conventional commits.","expected_files":["pyproject.toml"],"scored":true,"assertions":{"reader":{"match":"normalized"},"faithfulness":{"min_score":0.6}}}
