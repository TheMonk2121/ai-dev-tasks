=== FULL ABSTRACT AND INTRODUCTION ===
=== PAGE 1 ===
Applied Artiﬁcial Intelligence
An International Journal
ISSN: 0883-9514 (Print) 1087-6545 (Online) Journal homepage: www.tandfonline.com/journals/uaai20
Augmentation of Semantic Processes for Deep
Learning Applications
Maximilian Hoﬀmann, Lukas Malburg & Ralph Bergmann
To cite this article: Maximilian Hoﬀmann, Lukas Malburg & Ralph Bergmann (2025)
Augmentation of Semantic Processes for Deep Learning Applications, Applied Artiﬁcial
Intelligence, 39:1, 2506788, DOI: 10.1080/08839514.2025.2506788
To link to this article:  https://doi.org/10.1080/08839514.2025.2506788
© 2025 The Author(s). Published with
license by Taylor & Francis Group, LLC.
View supplementary material 
Published online: 02 Jun 2025.
Submit your article to this journal 
Article views: 1233
View related articles 
View Crossmark data
Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journalInformation?journalCode=uaai20

=== PAGE 2 ===
Augmentation of Semantic Processes for Deep Learning 
Applications
Maximilian Hoffmann
a,b, Lukas Malburg
a,b, and Ralph Bergmann
a,b
aArtificial Intelligence and Intelligent Information Systems, University of Trier, Trier, Germany; bGerman 
Research Center for Artificial Intelligence (DFKI), Branch University of Trier, Trier, Germany
ABSTRACT
The popularity of Deep Learning (DL) methods used in business 
process management research and practice is constantly 
increasing. One important factor that hinders the adoption of 
DL in certain areas is the availability of sufficiently large training 
datasets, particularly affecting domains where process models 
are mainly defined manually with a high knowledge-acquisition 
effort. In this paper, we examine process model augmentation in 
combination with semi-supervised transfer learning to enlarge 
existing datasets and train DL models effectively. The use case of 
similarity learning between manufacturing process models is 
discussed. Based on a literature study of existing augmentation 
techniques, a concept is presented with different categories of 
augmentation from knowledge-light approaches to knowledge- 
intensive ones, e. g. based on automated planning. Specifically, 
the impacts of augmentation approaches on the syntactic and 
semantic correctness of the augmented process models are 
considered. The concept also proposes a semi-supervised trans-
fer learning approach to integrate augmented and non- 
augmented process model datasets in a two-phased training 
procedure. The experimental evaluation investigates augmen-
ted process model datasets regarding their quality for model 
training in the context of similarity learning between manufac-
turing process models. The results indicate a large potential 
with a reduction of the prediction error of up to 53%.
ARTICLE HISTORY 
Received 9 September 2024  
Revised 13 March 2025  
Accepted 19 March 2025  
Introduction
Data-driven Artificial Intelligence (AI) methods such as Deep Learning (DL) 
have recently gained significant importance in practice and research. One of 
these research areas for the use of DL techniques is BPM (e.g., Di 
Francescomarino and Ghidini 2022; Nolle et al. 2018; Pfeiffer, Lahann, and 
Fettke 2021; Rama-Maneiro, Vidal, and Lama 2023). However, to fully utilize 
the DL techniques and their generalization capabilities, a rich set of training 
data with meaningful training examples is needed, which leads to considerable 
CONTACT Maximilian Hoffmann 
hoffmannm@uni-trier.de, 
Artificial Intelligence and Intelligent Information 
Systems, University of Trier, 54296 Trier, Germany
Supplemental data for this article can be accessed online at https://doi.org/10.1080/08839514.2025.2506788
APPLIED ARTIFICIAL INTELLIGENCE                    
2025, VOL. 39, NO. 1, e2506788 (48 pages) 
https://doi.org/10.1080/08839514.2025.2506788
© 2025 The Author(s). Published with license by Taylor & Francis Group, LLC.  
This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/ 
licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly 
cited. The terms on which this article has been published allow the posting of the Accepted Manuscript in a repository by the author(s) 
or with their consent.

=== PAGE 3 ===
effort for data acquisition (Hoffmann and Bergmann 2022b; Schuler et al.  
2023).
As this effort is not always manageable and sometimes training data is 
inaccessible, DL methods can be combined with other AI techniques in an 
approach commonly known as informed machine learning or hybrid AI (von 
Rueden et al. 2021). There, the disadvantage of not enough or imbalanced data 
is compensated by symbolic knowledge. A more common technique to miti-
gate these data issues in practice and research is data augmentation (Mumuni 
and Mumuni 2022; van Dyk, David, and Meng 2001; Zhao et al. 2022; Zhou 
et al. 2023). Data augmentation aims to increase the amount of available 
training data to make the DL models trained on it more accurate and robust. 
In domains such as image processing and Natural Language Processing (NLP) 
(e. g., Chen, Kornblith et al. 2020), elementary transformational augmenta-
tions (e.g., the rotation of an image or the replacement of words with syno-
nyms) or synthesis-based augmentations using generative models have proven 
effective. These techniques typically exploit the intrinsic structure of their 
respective data types, ensuring that the augmented examples remain consis-
tent with the original data distribution.
In contrast, in BPM and specifically for process models, the exploration of 
augmentation techniques is still in its infancy. Previous studies (e. g., de Leoni, 
van der Aalst, and Dees 2016; Käppel and Jablonski 2023; Käppel, Schönig, 
and Jablonski 2021; Venkateswaran et al. 2021) have introduced augmentation 
strategies that are largely tailored to specific domains or are limited to event 
log data. For instance, while traditional data augmentation methods focus 
primarily on simple and domain-agnostic transformations, these BPM- 
specific approaches often rely on ad-hoc modifications that do not generalize 
well across different types of process models or application contexts. 
Moreover, many of these studies do not address the challenge of preserving 
the semantic integrity of the process models during augmentation, a critical 
aspect to ensure that the augmented data remain useful for training robust DL 
models.
Our work aims to bridge this gap. The goal of this paper is to discuss 
approaches of process model augmentation in combination with an effective 
training method of DL models with augmented and non-augmented process 
models. The contributions of the paper are the following: 1) we present 
a literature study on process augmentation techniques that are currently 
available to be used in DL contexts, 2) we discuss three categories of augmen-
tation from knowledge-light approaches, i. e., deleting and replacing nodes 
and edges, to knowledge-intensive ones based on automated planning 
(Marrella 2019; McDermott et al. 1998), 3) we present a training procedure 
based on semi-supervised transfer learning (Kudenko 2014; Tan et al. 2018; 
Weiss, Khoshgoftaar, and Wang 2016) to train DL models effectively with 
augmented and non-augmented data, 4) we conduct an extensive evaluation in 
e2506788-2
M. HOFFMANN ET AL.

=== PAGE 4 ===
which we determine the suitability of different augmentation methods for 
improving the training procedure of DL models based on our previous work 
(Hoffmann et al. 2020; Schuler et al. 2023). Throughout the paper, we examine 
the task of learning similarities between process models from a manufacturing 
domain (Malburg, Klein, and Bergmann 2020, 2023; Seiger et al. 2022) as the 
main use case.
The paper is structured as follows: In Section 2, we describe the foundations 
that consist of the process model representation and the application domain 
applied in this work, the semantic similarity assessment between process 
models, and the basics of using graph neural networks for graph embedding. 
In addition, we present and discuss related augmentation approaches (see 
Section 3). Based on these foundations and the identified research gaps, 
a concept is presented to augment process models and use them to train DL 
models (see Section 4). To assess the utility of the concept for the addressed 
problems, an experimental evaluation is conducted by using several training 
data configurations for GNN based on the discussed augmentation techniques 
(see Section 5). In this context, we use several well-known metrics to evaluate 
the quality of the trained models for process model similarity learning. Finally, 
a conclusion is given, and future work is discussed in Section 7.
Foundations
In this section, we introduce the foundations of the semantic process model 
representation and the manufacturing domain with its underlying domain 
model that serves as an application scenario (see Section 2.1 and Section 2.2). 
Since the focus of the work is using DL models for similarity learning, 
Section 2.3 introduces how similarities are computed between workflows 
and Section 2.4 follows with GNN that are trained for this task. Finally, the 
proposed approach is embedded into a broader literature context with a focus 
on generic graph augmentation methods (see Section 3.1) and more specific 
approaches from BPM literature (see Section 3.2).
Semantic workflow representation and domain
Process models consist of several components that interact with each other to 
achieve the common goal of the process. For example, the main components 
of process models are activities that represent tasks or actions. These activities 
have relationships with other components such as the participants who per-
form the activity, the data used during the execution, etc. There are different 
ways of representing process models, e. g., BPMN, Petri Nets, or BPEL/WS- 
BPEL (Schultheis et al. 2024). This work uses semantically annotated directed 
graphs, referred to as NEST graphs (Bergmann and Gil 2014). The NEST graph 
format is a generic graph format that is well-suited for modeling processes, as 
APPLIED ARTIFICIAL INTELLIGENCE
e2506788-3

=== PAGE 5 ===
processes have an inherent graph structure of components and interrelations 
between them. The strength of NEST graphs is in modeling semantic informa-
tion with an underlying domain model, and they are well-integrated in our 
previous work (Malburg, Hoffmann and Bergmann 2023). To ensure compat-
ibility with other more common process model representation formats such as 
BPMN, we employ a converter from BPMN to NEST and back. The converter 
is available as part of the ProCAKE framework1 (Bergmann et al. 2019). All 
process models used in the experiments are represented in NEST and BPMN, 
created with this converter. See Section 5 and the statement on data availability 
at the end of the article for more information.
Each NEST graph is a quadruple G ¼ ðN; E; S; TÞ, defined by a set of nodes 
N, a set of edges E � N � N, a function T : N [ E ! T , assigning a type to 
each node and edge,2 and a function S : N [ E ! S, assigning a semantic 
description from a semantic metadata language (e. g., an ontology) to nodes 
and edges. While nodes and edges build the structure of each graph, types and 
semantic descriptions are employed to further capture semantic information. 
As a result, both nodes and edges always have a type and usually also have 
a semantic description.
The definition of NEST graphs is rather generic to allow their usage in 
diverse domains, e.g., to represent cooking recipes (Müller 2018), to solve 
constraint satisfaction problems (Grumbach and Bergmann 2017), to model 
scientific workflows (Zeyen, Malburg, and Bergmann 2019), to represent 
manufacturing processes (Malburg et al. 2020), or to define argumentation 
graphs (Lenz et al. 2019). As the process models addressed in this work are 
characterized by certain rules on top of the generic NEST graph definition, we 
introduce the concept of a sequential NEST process model:
A sequential NEST process model is a NEST graph with a sequential control-flow, 
a sequential data-flow, and without missing semantic annotations.
By a sequential control-flow, we mean that every task node in the process is 
connected to exactly one preceding task node and one following task node by 
a control-flow edge. There are two exceptions to this rule, i.e., the start task 
with only one outgoing control-flow edge and the end task with only one 
ingoing control-flow edge. The second important extension of NEST graphs 
toward sequential NEST process models is their sequential data-flow. This 
property is similar to the sequential control-flow but refers to data nodes: Each 
data node has to be produced by exactly one task and consumed by exactly one 
task. There is also the exception of two data nodes, i.e., at the beginning of the 
sequence and at the end, which are consumed or produced only, respectively. 
A sequential NEST process model with a sequential data-flow prohibits that, 
for instance, a task consumes or produces multiple data nodes. The third 
property of sequential NEST process models is about the semantic annotations 
of nodes and edges that must not be missing. This restriction contributes to 
e2506788-4
M. HOFFMANN ET AL.

=== PAGE 6 ===
the completeness of the process’ semantic knowledge and aims to prevent 
processes with nodes or edges that lack any form of semantic annotation and 
can only be characterized by their type.
The manufacturing application scenario that we utilize for this work is an 
example of a domain of sequential NEST process models. The scenario is 
based on process models that are executed in a smart factory3 (Malburg, Klein, 
and Bergmann 2020, 2023; Seiger et al. 2022). These processes are considered 
as cyber-physical workflows (Marrella et al. 2018; Seiger et al. 2019) because 
they are executed by actuators and are driven by sensors capturing the produc-
tion environment. Figure 1 illustrates an exemplary manufacturing process as 
a sequential NEST process model, representing the production steps for 
producing sheet metals. The illustrated process is a typical example of the 
processes used in this work. Task nodes represent concrete activities during 
production that are executed by actuators, i. e., machines, in the smart factory 
by a corresponding service. The state of the product produced is captured by 
the data nodes. Each data node represents the current production state of the 
sheet metal. The semantic descriptions of task and data nodes are used to 
describe the properties of these nodes. For example, the semantic description 
of Transport from Warehouse to Oven contains relevant parameters for con-
figuring the machine that executes it, e.g., the start and end position of the 
transport. Similarly, the semantic description of the first Sheet Metal data node 
is composed of the concrete position of the product, i.e., ov_1, and its 
characteristics, e. g., the size and thickness.
This example shows that the semantic annotations are a key aspect of the 
information a process holds. In particular, it shows the tight coupling of the 
syntactic information that is given by the representation as a sequential 
NEST process model, e. g., node and edge types and graph structure, and 
the semantic information that is expressed by the annotations of nodes and 
edges, e. g., the position of a workpiece. However, a process such as the one 
shown in Figure 1 that is consistent with all constraints of sequential NEST 
Figure 1. A NEST graph representing a sheet metal manufacturing process (based on: Malburg, 
Brand, and Bergmann 2023).
APPLIED ARTIFICIAL INTELLIGENCE
e2506788-5

=== PAGE 7 ===
process models and has well-formed semantic annotations according to the 
respective semantic metadata language is not guaranteed to be executable in 
the smart factory, i. e., the manufacturing environment. The reason for this 
is the inability of the semantic metadata language to describe the behavior 
of the process tasks in the environment during execution. For example, the 
task Burn in Figure 1 is parameterized by its semantic annotation to 
produce a middle-sized thick sheet metal. After being burned, the sheet 
metal is transported to the first HBW to be stored there. While the 
semantic annotations of all the tasks involved are within the definitions 
of the semantic metadata language, there is still room for errors. It might 
be possible that the first VGR is only capable of moving small-sized work-
pieces, or that the shop floor layout could prevent the first VGR from 
reaching the designated storage buckets in the first HBW. Both problems 
could be solved by using the second VGR that transports the workpiece to 
the second HBW.
Domain model
An additional knowledge representation, called domain model, is utilized to 
check if a sequential NEST process model is executable in its designated 
manufacturing environment. The domain model also allows interoperability 
of a process with other systems of the application context, e. g., ERP systems, 
MES, or WfMS (Seiger et al. 2022). The complexity and completeness of the 
information provided by the domain model can vary greatly and depend on 
the respective domain.
Listing 1 Planning Action for the Burn Service.
e2506788-6
M. HOFFMANN ET AL.

=== PAGE 8 ===
While rather simple domains might have simpler domain models, more 
complex domains might also have more complex domain models. The model 
used in this work is given as a comprehensive planning domain description 
based on the PDDL (McDermott et al. 1998). The planning domain model 
consists of the available actions of the smart factory enhanced with semantic 
information about the preconditions that must be satisfied for execution and 
the effects that hold after a successful execution. We use a semantic service- 
based architecture for the smart factory (see Malburg, Klein, and Bergmann  
2020; Seiger et al. 2022) and, based on this architecture, a domain expert 
creates the planning actions contained in the domain description. Listing 1 
illustrated a planning action created by the domain expert for the Burn service. 
A planning action in PDDL consists of an action name (Line 1), parameters for 
configuring the action (Line 2), preconditions that need to be satisfied for 
execution (Lines 3–8), and effects that represent the state transition after 
successful execution (Lines 9–14). Each task executed by a service in the 
smart factory has several parameters to be configured (see Section 2.1). For 
the illustrated Burn task, the executing resource, the size of the burned sheet 
metal, and the thickness must be specified. In this context, it is important to 
note that the process ID is a reference to the corresponding process from the 
smart factory. The preconditions specify the world state in which the action 
can be executed. For the Burn service, the workpiece must be at the location of 
the oven (Line 5). In addition, the oven must be ready and not inactive (Lines 
6–7) as well as the workpiece must be a steel slab, i. e., it must not be burned 
already (Line 8). After executing the action, the state of the product changes: 
the workpiece is no steel slab anymore but a sheet metal with a certain size and 
thickness (Lines 9–13). To capture the costs for executing an action with 
certain parameters, the total cost function is increased by a predefined value 
for the service executed (Line 14).
The information in the domain model allows us to check whether a process 
model can be executed successfully by considering the defined preconditions 
and effects. It also allows the generation of process models by using automated 
planning (Ghallab, Nau, and Traverso 2016; Haslum et al. 2019), which is 
applied for process model augmentation in this paper.
Semantic workflow similarity
The combination of syntax and semantics of sequential NEST process models 
shows the complexity that is involved with all applications working with this 
data. One task that arises in these applications is the comparison of process 
models, e. g., for finding a replacement in case the execution of a different 
process model fails (Malburg, Brand, and Bergmann 2023; Malburg, 
Hoffmann, and Bergmann 2023). To compare different process models, simi-
larities are a common measurement. We also use similarity computation as 
APPLIED ARTIFICIAL INTELLIGENCE
e2506788-7

=== PAGE 9 ===
a demonstrator in the experiments of the paper, as it is widely used in different 
fields of AI. One example is CBR: In the problem-solving methodology of 
CBR, similarities are widely used to determine which process models are alike 
and can help to solve emerging problems. Thereby, a repository of process 
models is queried with a different process model to find the best-matching 
process models of the repository. This is useful since the found process models 
might be better suited to solve the problem at hand, e. g., a sudden failure of 
a machine, than the currently executed process model.
There are various methods for calculating similarities between process 
models. These methods are sometimes consolidated under the term business 
process matching (Weidlich, Remco, and Jan 2010). Schoknecht et al. (2017) 
categorize different methods based on seven characteristics, e. g., objective and 
implementation, and, thus, provide a comprehensive overview of the available 
literature. We use the semantic similarity measure proposed by Bergmann and 
Gil (2014) as it is specifically tailored for NEST graphs and focuses on 
semantic similarity measures on a node-level, which fits our use case.
To compute the similarity between two NEST graphs, the similarity measure 
evaluates the structure of nodes and edges, as well as the semantic descriptions 
and types of these components. The final similarity at the graph level is deter-
mined by the similarities between the nodes and edges of both graphs (Richter  
2007). Therefore, a global similarity, denoting the similarity between two graphs, 
consists of local similarities, i. e., the pairwise similarities of nodes and edges. 
The similarity between two identical types of nodes is defined as the similarity of 
the semantic descriptions of these nodes. The similarity value between two edges 
of the same type includes not only the similarity between the respective semantic 
descriptions but also the similarity of the connected nodes. The similarities of 
the semantic descriptions are computed based on the underlying domain model. 
For instance, in the given exemplary process model illustrated in Figure 1, the 
similarity between two task nodes would include further similarities between 
their parameters. This procedure requires the definition of similarity measures 
for all components of the semantic descriptions as part of the domain’s similarity 
knowledge. These metrics are usually designed with the input of experts in the 
field and consider the particular data types in use. For example, they could utilize 
word embeddings (Mikolov et al. 2013) for textual data or the MAE for 
numerical values (Bergmann 2002).
To determine the global similarity between a pair of graphs from the local 
similarities of the nodes and edges, an injective partial mapping is computed. 
The objective of this mapping is to maximize the aggregated local similarities 
between all mapped pairs of corresponding nodes and edges. Nonetheless, as 
the number of nodes and edges gets larger, the complexity of finding this 
mapping increases substantially, given that each node and edge can potentially 
be mapped to multiple counterparts. To address this complexity, Bergmann 
and Gil (2014) propose using an A* search algorithm. A* search is faster than 
e2506788-8
M. HOFFMANN ET AL.

=== PAGE 10 ===
exhaustive search, but typically still requires a considerable amount of time 
and, thus, can result in a slow similarity computation (Hoffmann and 
Bergmann 2022b; Hoffmann et al. 2020, 2022; Klein, Malburg, and 
Bergmann 2019; Ontañón 2020; Zeyen and Bergmann 2020).
Graph embedding
Previous work by Hoffmann and Bergmann (2022b) tackles the problem of slow 
similarity computations by using an automatically learned similarity measure 
based on a Siamese GNN, denoted as the GEM. The basic idea is to learn to 
predict the pairwise similarities of workflows with the GEM for faster and some-
times even more accurate similarity computations (Hoffmann and Bergmann  
2022b). Therefore, the GEM transforms the graph structure and the semantic 
annotations and types of all nodes and edges into a whole-graph latent vector 
representation. These vectors are then combined to calculate a similarity value. An 
overview of the used GNN architecture is illustrated in Figure 2.
The GEM follows a general architecture composed of four parts: First, the 
embedder starts by transforming the node and edge features into initial 
embeddings within a vector space. These features encompass semantic anno-
tations and types, and they undergo a specialized encoding and processing 
procedure tailored for semantic graphs (for more details, see). Next, the 
propagation layer collects information for each node from its local surround-
ings by transmitting messages, as described by Gilmer et al. (2017). In detail, 
the vector representation of a node is continuously updated by incorporating 
the vectors of neighboring nodes linked through incoming edges. This itera-
tive process is repeated multiple times. Following that, the aggregator com-
bines the node embeddings at this stage to generate a vector representation of 
the entire graph. Eventually, the similarity between two graphs is calculated by 
Figure 2. The GEM (Source: Hoffmann et al. 2020).
APPLIED ARTIFICIAL INTELLIGENCE
e2506788-9

