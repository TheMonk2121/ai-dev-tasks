// Simple LM Studio status checker
const axios = require('axios');

console.log('ğŸ” Checking Ollama Status...\n');

async function checkStatus() {
    try {
        // Check server connection
        const response = await axios.get('http://localhost:11434/api/tags', {
            timeout: 3000
        });
        
        console.log('âœ… Ollama server is running');
        console.log(`ğŸ“‹ Available models: ${response.data.models?.length || 0}`);
        
        // Check for Mistral model
        const models = response.data.models || [];
        const mistralModel = models.find(model => 
            model.name.toLowerCase().includes('mistral')
        );
        
        if (mistralModel) {
            console.log(`âœ… Mistral model found: ${mistralModel.name}`);
            console.log('\nğŸ‰ Ollama is ready for integration!');
            console.log('\nğŸ“‹ Next Steps:');
            console.log('1. Run: node test_lm_studio_integration.js');
            console.log('2. Test the extension with Cursor IDE');
            console.log('3. Configure extension settings');
            return true;
        } else {
            console.log('âŒ Mistral model not found');
            console.log('ğŸ’¡ Available models:', models.map(m => m.name).join(', '));
            console.log('\nğŸ“‹ To complete setup:');
            console.log('1. Load Mistral model: ollama pull mistral');
            console.log('2. Run this check again');
            return false;
        }
        
    } catch (error) {
        console.log('âŒ Ollama server not responding');
        console.log('\nğŸ“‹ To start Ollama server:');
        console.log('1. Open terminal');
        console.log('2. Run: ollama serve');
        console.log('3. Run this check again');
        return false;
    }
}

// Run the check
checkStatus().catch(console.error); 