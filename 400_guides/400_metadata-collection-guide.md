<!-- DATABASE_SYNC: REQUIRED -->

<!-- CONTEXT_REFERENCE: 400_guides/400_cursor-context-engineering-guide.md -->

<!-- MODULE_REFERENCE: 400_guides/400_performance-optimization-guide.md -->

<!-- MODULE_REFERENCE: docs/100_ai-development-ecosystem.md -->

<!-- MEMORY_CONTEXT: MEDIUM - Metadata collection and analysis -->

# ðŸ“Š Metadata Collection Guide

## ðŸ“Š Metadata Collection Guide

{#tldr}

## ðŸ”Ž TL;DR

| what this file is | read when | do next |
|-------------------|-----------|---------|
|                   |           |         |

- **what this file is**: Quick summary of ðŸ“Š Metadata Collection Guide.

- **read when**: When you need a fast orientation or before using this
  file in a workflow.

- **do next**: Scan the headings below and follow any â€˜Quick Startâ€™ or
  â€˜Usageâ€™ sections.

## ðŸŽ¯ **Current Status**-**Status**: âœ… **ACTIVE**- Metadata collection system operational

- **Priority**: âš¡ High - Data collection and analysis

- **Points**: 4 - Moderate complexity, ongoing maintenance

- **Dependencies**: 400_guides/400_cursor-context-engineering-guide.md,
  400_guides/400_performance-optimization-guide.md

- **Next Steps**: Expand collection coverage and optimize analysis

}

``` markdown

- *Collection Method**: `scripts/backlog_parser.py` - Robust parser with regex patterns and HTML comment extraction.

### **2. Execution State Database (`task_execution.db`)**

- *Primary Data Source**: SQLite database for persistent state tracking and execution history.

- *Database Schema**:

```sql
- - Task execution records
task_executions:
  task_id TEXT PRIMARY KEY,
  status TEXT NOT NULL,
  started_at TIMESTAMP NOT NULL,
  completed_at TIMESTAMP,
  error_message TEXT,
  retry_count INTEGER DEFAULT 0,
  progress REAL DEFAULT 0.0,
  execution_time REAL,
  metadata TEXT,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP

- - Task metadata storage
task_metadata:
  task_id TEXT PRIMARY KEY,
  title TEXT NOT NULL,
  priority TEXT NOT NULL,
  points INTEGER NOT NULL,
  description TEXT,
  tech_footprint TEXT,
  dependencies TEXT,
  score_total REAL,
  human_required BOOLEAN DEFAULT FALSE,
  human_reason TEXT,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP

- - Detailed execution history
execution_history:
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  task_id TEXT NOT NULL,
  status TEXT NOT NULL,
  timestamp TIMESTAMP NOT NULL,
  message TEXT,
  metadata TEXT,
  FOREIGN KEY (task_id) REFERENCES task_executions (task_id)

- - Performance metrics
performance_metrics:
  task_id TEXT PRIMARY KEY,
  avg_execution_time REAL,
  success_rate REAL,
  total_executions INTEGER DEFAULT 0,
  successful_executions INTEGER DEFAULT 0,
  failed_executions INTEGER DEFAULT 0,
  last_execution TIMESTAMP,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP

```markdown

- *Collection Method**: `scripts/state_manager.py` - Comprehensive state management with automatic metrics calculation.

### **3. Error Handling System**

- **Primary Data Source**: Error classification, recovery actions, and retry statistics.
- **Error Categories**: See
[`400_guides/400_comprehensive-coding-best-practices.md`](400_guides/400_comprehensive-coding-best-practices.md) for
comprehensive error handling patterns and implementation examples.
- **Error Severity Levels**: See comprehensive guide for severity classification and handling strategies.
- **Collected Error Metadata**:
  - **Error Classification**: Type, severity, category, context
  - **Recovery Actions**: Automatic recovery procedures attempted
  - **Retry Statistics**: Retry counts, delays, success rates
  - **Context Information**: System info, stack traces, timestamps
  - **Error Patterns**: Frequency analysis and trend detection
- **Collection Method**: `scripts/error_handler.py` - Advanced error handling with automatic classification and
recovery.

## ðŸŽ¯ **Metadata Usage Patterns**###**1. Intelligent Task Prioritization**

- **Usage**: Automatically prioritize tasks based on multiple factors.
- **Implementation**: See
[`400_guides/400_comprehensive-coding-best-practices.md`](400_guides/400_comprehensive-coding-best-practices.md) for
comprehensive task prioritization patterns and implementation examples.
- **Metadata Factors**:
  - **Priority Level**: ðŸ”¥ (Critical), ðŸ“ˆ (High), â­ (Medium), ðŸ”§ (Low)
  - **Score Total**: Combined business value, technical complexity, risk reduction
  - **Dependencies**: Prerequisite task completion status
  - **Human Requirements**: Tasks requiring human input flagged appropriately

### **2. Performance Optimization**

- **Usage**: Track and optimize execution performance based on historical data.
- **Implementation**: See
[`400_guides/400_comprehensive-coding-best-practices.md`](400_guides/400_comprehensive-coding-best-practices.md) for
comprehensive performance optimization patterns and implementation examples.

- *Key Metrics**:

- **Average Execution Time**: Historical execution duration

- **Success Rate**: Percentage of successful executions

- **Retry Patterns**: Frequency and timing of retries

- **Resource Usage**: Memory, CPU, and I/O patterns

### **3. Error Recovery & Prevention**

- *Usage**: Automatic error classification and recovery procedures.

- *Implementation**:

```python

## Classify errors automatically

error_category = error_handler.classify_error(error)
severity = error_handler.determine_severity(error, context)

## Apply appropriate recovery

recovery_action = error_handler._get_recovery_action(error_info)

```markdown

- *Recovery Actions**:

- **Network Errors**: Retry with exponential backoff

- **File System Errors**: Cleanup temporary files and retry

- **Database Errors**: Reconnect and retry

- **Permission Errors**: Attempt permission fixes

- **Timeout Errors**: Extend timeout and retry

## **4. Dependency Management**

- *Usage**: Validate and track task dependencies to ensure proper execution order.

- *Implementation**:

```python

## Validate dependencies before execution

missing_deps = engine.validate_dependencies()
for task_id, deps in missing_deps.items():
    logger.warning(f"Task {task_id} missing dependencies: {deps}")

```markdown

- *Dependency Tracking**:

- **Prerequisite Tasks**: Tasks that must complete first

- **Dependency Status**: Real-time completion tracking

- **Circular Dependency Detection**: Prevent dependency loops

- **Dependency Chains**: Multi-level dependency resolution

## **5. Progress Tracking & Reporting**

- *Usage**: Real-time progress monitoring and comprehensive reporting.

- *Implementation**:

```python

## Real-time status updates

status = engine.get_status()
print(f"Total tasks: {status['total_tasks']}")
print(f"Pending tasks: {status['pending_tasks']}")
print(f"Success rate: {status['success_rate']}%")

```markdown

- *Progress Metrics**:

- **Execution Status**: Running, completed, failed, pending

- **Progress Percentage**: Real-time completion progress

- **Time Estimates**: Based on historical execution data

- **Resource Utilization**: Current system resource usage

## ðŸ“ˆ **Advanced Analytics Capabilities**###**1. Task Performance Analysis**

- *Usage**: Generate comprehensive performance reports and identify optimization opportunities.

- *Implementation**:

```python

## Generate performance reports

stats = state_manager.get_statistics()
print(f"Average execution time: {stats['avg_execution_time']:.2f}s")
print(f"Overall success rate: {stats['success_rate']:.1f}%")
print(f"Total retries: {stats['total_retries']}")

```markdown

- *Analytics Metrics**:

- **Execution Time Analysis**: Average, median, 95th percentile

- **Success Rate Trends**: Historical success rate patterns

- **Retry Analysis**: Frequency and effectiveness of retries

- **Resource Efficiency**: CPU, memory, and I/O utilization

## **2. Error Pattern Recognition**

- *Usage**: Analyze error trends and implement preventive measures.

- *Implementation**:

```python

## Analyze error trends

error_stats = error_handler.get_error_statistics()
print(f"Most common error category: {max(error_stats['by_category'])}")
print(f"Recovery success rate: {error_stats['recovery_success_rate']:.1f}%")

```markdown

- *Pattern Analysis**:

- **Error Frequency**: Most common error types and categories

- **Recovery Success Rate**: Effectiveness of automatic recovery

- **Error Severity Distribution**: Critical vs. non-critical errors

- **Temporal Patterns**: Error occurrence timing and frequency

## **3. Resource Optimization**

- *Usage**: Identify performance bottlenecks and optimize resource allocation.

- *Implementation**:

```python

## Identify bottlenecks

slow_tasks = [t for t in tasks if t.avg_execution_time > 300]
high_retry_tasks = [t for t in tasks if t.retry_count > 3]

```markdown

- *Optimization Metrics**:

- **Slow Task Identification**: Tasks exceeding performance thresholds

- **High Retry Tasks**: Tasks with excessive retry attempts

- **Resource Usage Patterns**: Memory and CPU utilization trends

- **Capacity Planning**: Predict future resource requirements

## **4. Predictive Analytics**

- *Usage**: Predict task success and execution time based on historical data.

- *Implementation**:

```python

## Predict task success based on historical data

def predict_success(task_id):
    history = state_manager.get_execution_history(task_id)
    success_rate = calculate_success_rate(history)
    avg_time = calculate_avg_time(history)
    return success_rate > 0.8 and avg_time < 600

```markdown

- *Prediction Models**:

- **Success Probability**: Likelihood of task completion

- **Execution Time Prediction**: Estimated completion time

- **Resource Requirement Prediction**: Expected resource usage

- **Risk Assessment**: Probability of errors or failures

## ðŸ”„ **Metadata Flow Architecture**```

000_core/000_backlog.md â†’ BacklogParser â†’ Task Objects
                    â†“
Task Objects â†’ StateManager â†’ SQLite Database
                    â†“
Execution Engine â†’ ErrorHandler â†’ Recovery Actions
                    â†“
Performance Metrics â†’ Analytics â†’ Optimization

```markdown**Data Flow Stages**:
1. **Collection**: Extract metadata from various sources
2. **Processing**: Parse, validate, and structure data
3. **Storage**: Persist data in appropriate databases
4. **Analysis**: Generate insights and patterns
5. **Optimization**: Apply insights to improve performance

## ðŸš€ **Real-World Usage Examples**###**1. Automated Sprint Planning**

- *Command**: `python3 scripts/process_tasks.py auto --max-tasks 5`

- *Process**:
1. Parse backlog for pending tasks
2. Sort by priority and score
3. Validate dependencies
4. Execute top 5 tasks automatically
5. Track execution progress and results

### **2. Performance Monitoring**

- *Command**: `python3 scripts/process_tasks.py status`

- *Output**:
```

Execution Status: Total tasks: 56 Pending tasks: 0

Status breakdown: completed: 28 pending: 0 failed: 0

Recent executions: B-049: completed (2024-08-07T17:00:00) B-072:
completed (2024-08-07T16:15:00)

``` yaml

### **3. Error Analysis**

- *Command**: `python3 scripts/error_handler.py --export-report error_analysis.json`

- *Report Contents**:

- Error frequency by category

- Recovery success rates

- Retry pattern analysis

- Severity distribution

- Temporal error trends

### **4. Dependency Validation**

- *Command**: `python3 scripts/process_tasks.py validate`

- *Output**:
```

All dependencies are satisfied

``` text

or
```

Missing dependencies found: B-050: B-049 B-051: B-050

``` yaml

## ðŸ”® **Future Metadata Enhancements**###**1. Machine Learning Integration**

- *Predictive Success Modeling**:

- Use historical data to predict task success probability

- Implement ML-based resource allocation optimization

- Develop anomaly detection for unusual execution patterns

- *Implementation Ideas**:

```python

# ML-based success prediction

def predict_task_success(task_features):
    model = load_ml_model('success_predictor.pkl')
    return model.predict(task_features)

# Anomaly detection

def detect_execution_anomalies(execution_data):
    anomalies = anomaly_detector.detect(execution_data)
    return anomalies

```yaml

## **2. Advanced Analytics**

- *Trend Analysis**:

- Long-term performance trend identification

- Correlation analysis between task types and success rates

- Capacity planning based on historical patterns

- *Correlation Analysis**:

```python

# Find relationships between task characteristics and success

def analyze_task_correlations():
    correlations = {
        'priority_vs_success': calculate_correlation(priority, success_rate),
        'complexity_vs_time': calculate_correlation(complexity, execution_time),
        'dependencies_vs_retries': calculate_correlation(dependency_count, retry_count)
    }
    return correlations
```

## **3. Real-time Monitoring**

- \*Live Dashboards\*\*:

- Real-time execution monitoring with live updates

- Proactive error detection and alerting

- Performance threshold monitoring and notifications

- \*Dashboard Features\*\*:

- Live task execution status

- Real-time performance metrics

- Error rate monitoring

- Resource utilization tracking

## ðŸ“‹ **Best Practices**

## âš¡ Quick reference

- Key sources: `000_core/000_backlog.md` (scores, deps, status),
  `task_execution.db` (executions, perf, retries), error system
  (categories, severity, recovery)

- Common commands:

  - Performance stats: `python3 scripts/state_manager.py --stats`
  - Error stats: `python3 scripts/error_handler.py --stats`
  - Validate deps: `python3 scripts/process_tasks.py validate`
  - Export tasks:
    `python3 scripts/process_tasks.py list --format json > tasks.json`

- Export metrics (CSV):
  `python3 scripts/state_manager.py --stats --export-csv metrics.csv`

### **1. Data Quality**-**Validation**: Ensure all metadata is properly validated before storage

- **Consistency**: Maintain consistent data formats across all sources

- **Completeness**: Capture all relevant metadata fields for
  comprehensive analysis

### **2. Performance Optimization**-**Indexing**: Proper database indexing for fast queries

- **Caching**: Cache frequently accessed metadata

- **Compression**: Compress historical data to save storage space

### **3. Privacy & Security**-**Access Control**: Implement proper access controls for sensitive metadata

- **Data Retention**: Define clear data retention policies

- **Audit Logging**: Log all metadata access and modifications

### **4. Scalability**-**Horizontal Scaling**: Design for horizontal scaling as data grows

- **Partitioning**: Partition large datasets for better performance

- **Archiving**: Archive old data to maintain performance

## ðŸŽ¯ **Conclusion**

The metadata collection and analytics system provides a comprehensive
foundation for intelligent automation, performance optimization, and
data-driven decision making in the AI development ecosystem. By
leveraging this rich metadata, teams can:

- **Optimize Task Execution**: Use historical data to improve execution
  strategies

- **Predict Performance**: Anticipate issues before they occur

- **Automate Decision Making**: Let data drive prioritization and
  resource allocation

- **Continuous Improvement**: Use analytics to identify and implement
  improvements

This system transforms the development process from reactive to
proactive, enabling teams to build more efficiently and reliably.

------------------------------------------------------------------------

- \*Document Version\*\*: 1.0
- \*Last Updated\*\*: 2024-08-07
- **Next Review**: Monthly Review Cycle

<!-- README_AUTOFIX_START -->

## Auto-generated sections for 400_metadata-collection-guide.md

## Generated: 2025-08-18T08:03:22.754705

## Missing sections to add:

## Last Reviewed

2025-08-18

## Owner

Documentation Team

## Purpose

Describe the purpose and scope of this document

## Usage

Describe how to use this document or system

<!-- README_AUTOFIX_END -->
