# file: /Users/danieljacobs/Code/ai-dev-tasks/scripts/evaluation/improved_ab_testing_framework.py
# hypothesis_version: 6.138.15

[0.0, 0.05, 100.0, 300, 1000, '%Y-%m-%d %H:%M:%S', '+', '--baseline', '--max-workers', '--output-dir', '--tests', '--timeout', '0', '1', '50', '=', 'DSPY_MODEL', 'EVAL_DRIVER', 'EVAL_PROFILE', 'Output directory', 'RERANK_ENABLE', 'RETR_TOPK_BM25', 'RETR_TOPK_VEC', 'Timeout in seconds', '__main__', 'analysis', 'avg_f1_improvement', 'baseline_config', 'baseline_metrics', 'baseline_name', 'best_f1_improvement', 'best_test', 'cases', 'comparisons', 'dspy_rag', 'duration', 'error', 'f1', 'f1_improvement', 'f1_significant', 'failed', 'failed_tests', 'gold', 'latency_change', 'latency_ms', 'message', 'metrics/ab_tests', 'mock', 'overall_healthy', 'overall_significant', 'precision', 'precision_lift_pack', 'recall', 'recall_improvement', 'recall_significant', 'results', 'significance', 'status', 'success', 'success_rate', 'successful_tests', 'summary', 'synthetic', 'test_configs', 'test_metrics', 'test_name', 'timestamp', 'total_tests', 'w', 'ðŸ§ª A/B TEST SUMMARY']