#!/usr/bin/env python3
"""
Test Error Handling (T4)

Test to verify improved error handling for memory rehydrator failures.
"""

import os
import sys
import time

# Add src to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), "src"))

from cursor_integration import quick_task


def test_normal_operation():
    """Test normal operation with working memory rehydrator"""
    print("ğŸ” Test 1: Normal Operation")
    print("-" * 50)

    task = "Explain Python best practices"

    try:
        start_time = time.time()
        result = quick_task(task)
        execution_time = time.time() - start_time

        print(f"âœ… Normal operation completed in {execution_time:.2f}s")
        print(f"ğŸ“ Result preview: {result[:200]}...")

        # Check if result contains meaningful content
        if len(result) > 100 and any(word in result.lower() for word in ["python", "best", "practice", "code"]):
            print("   âœ… Result contains meaningful content")
            return True
        else:
            print("   âš ï¸ Result may be using fallback context")
            return False

    except Exception as e:
        print(f"âŒ Normal operation failed: {str(e)}")
        return False


def test_fallback_context():
    """Test fallback context quality"""
    print("\nğŸ” Test 2: Fallback Context Quality")
    print("-" * 50)

    # Import directly to test fallback function
    try:
        from dspy_modules.model_switcher import _get_fallback_context

        roles = ["coder", "planner", "implementer", "researcher", "reviewer"]
        task = "Test task for fallback context"

        for role in roles:
            fallback = _get_fallback_context(role, task)

            print(f"\nğŸ“‹ {role.upper()} Role:")
            print(f"   Context length: {len(fallback)} characters")
            print(f"   Preview: {fallback[:100]}...")

            # Check fallback quality
            quality_indicators = ["guidelines", "best practices", "standards", "focus", "approach"]
            found_indicators = sum(1 for indicator in quality_indicators if indicator.lower() in fallback.lower())

            if found_indicators >= 3:
                print(f"   âœ… High quality fallback ({found_indicators}/5 indicators)")
            elif found_indicators >= 2:
                print(f"   âš ï¸ Moderate quality fallback ({found_indicators}/5 indicators)")
            else:
                print(f"   âŒ Low quality fallback ({found_indicators}/5 indicators)")

        return True

    except Exception as e:
        print(f"âŒ Fallback context test failed: {str(e)}")
        return False


def test_retry_mechanism():
    """Test retry and progressive timeout behavior"""
    print("\nğŸ” Test 3: Retry Mechanism")
    print("-" * 50)

    # This test verifies the retry logic is implemented
    try:
        from dspy_modules.model_switcher import get_context_for_role

        # Test with a complex task that might trigger retries
        task = "Generate a comprehensive analysis of this complex AI system architecture"
        role = "researcher"

        start_time = time.time()
        context = get_context_for_role(role, task)
        execution_time = time.time() - start_time

        print(f"âœ… Context retrieval completed in {execution_time:.2f}s")
        print(f"ğŸ“ Context preview: {context[:200]}...")

        # Check if context was retrieved
        if "PROJECT CONTEXT" in context:
            print("   âœ… Context retrieval successful")

            if "FALLBACK" in context:
                print("   âš ï¸ Using fallback context (memory rehydrator may have failed)")
            else:
                print("   âœ… Using full memory rehydrator context")

            return True
        else:
            print("   âŒ No context retrieved")
            return False

    except Exception as e:
        print(f"âŒ Retry mechanism test failed: {str(e)}")
        return False


def test_failure_tracking():
    """Test failure tracking and circuit breaker behavior"""
    print("\nğŸ” Test 4: Failure Tracking")
    print("-" * 50)

    try:
        from dspy_modules.model_switcher import _failure_count, _max_failures

        print("ğŸ“Š Current failure tracking:")
        print(f"   Max failures threshold: {_max_failures}")
        print(f"   Current failure counts: {dict(_failure_count)}")

        if len(_failure_count) == 0:
            print("   âœ… No failures recorded - system healthy")
        else:
            for role, count in _failure_count.items():
                if count >= _max_failures:
                    print(f"   âš ï¸ Role {role} has {count} failures (threshold exceeded)")
                else:
                    print(f"   ğŸ“‹ Role {role} has {count} failures (within threshold)")

        return True

    except Exception as e:
        print(f"âŒ Failure tracking test failed: {str(e)}")
        return False


def main():
    """Run error handling tests"""
    print("ğŸ›¡ï¸ Error Handling Test Suite (T4)")
    print("=" * 80)

    tests = [
        ("Normal Operation", test_normal_operation),
        ("Fallback Context Quality", test_fallback_context),
        ("Retry Mechanism", test_retry_mechanism),
        ("Failure Tracking", test_failure_tracking),
    ]

    results = []

    for test_name, test_func in tests:
        print(f"\nğŸ§ª Running: {test_name}")
        result = test_func()
        results.append((test_name, result))

    # Summary
    print("\n" + "=" * 80)
    print("ğŸ“Š ERROR HANDLING TEST SUMMARY")
    print("=" * 80)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    print(f"ğŸ¯ Tests passed: {passed}/{total}")

    for test_name, result in results:
        status = "âœ… PASSED" if result else "âŒ FAILED"
        print(f"   {status}: {test_name}")

    print("\nğŸ›¡ï¸ Error Handling Features:")
    print("   â€¢ Retry logic with progressive backoff")
    print("   â€¢ Failure tracking and circuit breaker")
    print("   â€¢ Role-specific fallback contexts")
    print("   â€¢ Timeout management and optimization")
    print("   â€¢ Comprehensive error logging")

    if passed == total:
        print("\nğŸ‰ All error handling tests passed!")
    else:
        print(f"\nâš ï¸ {total - passed} test(s) failed - review error handling implementation")


if __name__ == "__main__":
    main()
