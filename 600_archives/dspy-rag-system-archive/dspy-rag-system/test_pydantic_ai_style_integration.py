#!/usr/bin/env python3
"""
B-1007 Integration Test
Validates that all Pydantic AI Style Enhancement components work together
"""

import time
from typing import cast

from src.dspy_modules.constitution_validation import (
    ConstitutionAwareValidator,
    ConstitutionValidator,
    ProgramOutput,
    create_default_constitution_ruleset,
)
from src.dspy_modules.context_models import (
    AIRole,
    CoderContext,
    ContextFactory,
    PlannerContext,
)
from src.dspy_modules.error_taxonomy import (
    ConstitutionErrorMapper,
    ErrorClassifier,
    ErrorFactory,
    ErrorSeverity,
)


def test_b1007_integration():
    """Test complete B-1007 integration"""

    print("üß™ Testing B-1007 Pydantic AI Style Enhancements Integration")
    print("=" * 70)

    # Test 1: Context Models
    print("\n1. Testing Role-Based Context Models...")

    # Create planner context
    planner_context = cast(
        PlannerContext,
        ContextFactory.create_context(
            AIRole.PLANNER,
            session_id="integration-test-session",
            project_scope="Implement Pydantic AI style enhancements for DSPy system with comprehensive validation",
            backlog_priority="P1",
            strategic_goals=["Type safety", "Error handling", "Constitution compliance"],
        ),
    )

    print(f"   ‚úÖ Planner context created: {planner_context.role.value}")
    print(f"   ‚úÖ Project scope: {planner_context.project_scope[:50]}...")
    print(f"   ‚úÖ Strategic goals: {len(planner_context.strategic_goals)} goals")

    # Create coder context
    coder_context = cast(
        CoderContext,
        ContextFactory.create_context(
            AIRole.CODER,
            session_id="integration-test-session",
            codebase_path="/tmp",
            language="python",
            file_context=["test_file.py"],
        ),
    )

    print(f"   ‚úÖ Coder context created: {coder_context.role.value}")
    print(f"   ‚úÖ Language: {coder_context.language}")

    # Test 2: Error Taxonomy
    print("\n2. Testing Error Taxonomy...")

    # Create various error types
    validation_error = ErrorFactory.create_validation_error(
        message="Field validation failed during context creation",
        field_name="project_scope",
        expected_value="non-empty string",
        actual_value="",
    )

    coherence_error = ErrorFactory.create_coherence_error(
        message="Role context mismatch detected",
        conflicting_elements=["planner", "coder"],
        coherence_rule="single_role_per_session",
    )

    dependency_error = ErrorFactory.create_dependency_error(
        message="Missing required dependencies",
        missing_dependencies=["pydantic", "dspy"],
        dependency_type="python_module",
    )

    print(f"   ‚úÖ Validation error created: {validation_error.error_type.value}")
    print(f"   ‚úÖ Coherence error created: {coherence_error.error_type.value}")
    print(f"   ‚úÖ Dependency error created: {dependency_error.error_type.value}")

    # Test error classification
    errors = [validation_error, coherence_error, dependency_error]
    error_stats = ConstitutionErrorMapper.get_error_classification_stats(errors)
    print(f"   ‚úÖ Error classification: {error_stats}")

    # Test 3: Constitution Validation
    print("\n3. Testing Constitution-Aware Validation...")

    # Create default constitution ruleset
    ruleset = create_default_constitution_ruleset()
    validator = ConstitutionValidator(ruleset)
    aware_validator = ConstitutionAwareValidator(validator)

    print(f"   ‚úÖ Default ruleset created with {len(ruleset.rules)} rules")

    # Test program output validation
    compliant_output = ProgramOutput(
        output_content="This is a comprehensive and detailed output that should pass all constitution validation rules including content length, role appropriateness, and security checks.",
        output_type="text",
        context=planner_context,
        constitution_compliance=None,
    )

    validated_output = aware_validator.validate_program_output(compliant_output)

    if validated_output.constitution_compliance:
        print(f"   ‚úÖ Compliant output validation: {validated_output.constitution_compliance.is_compliant}")
        print(f"   ‚úÖ Compliance score: {validated_output.constitution_compliance.compliance_score:.2f}")
    else:
        print("   ‚ùå No compliance data available")

    # Test non-compliant output
    non_compliant_output = ProgramOutput(
        output_content="Short", output_type="text", context=coder_context, constitution_compliance=None
    )

    validated_non_compliant = aware_validator.validate_program_output(non_compliant_output)

    if validated_non_compliant.constitution_compliance:
        print(f"   ‚úÖ Non-compliant output validation: {validated_non_compliant.constitution_compliance.is_compliant}")
        print(f"   ‚úÖ Compliance score: {validated_non_compliant.constitution_compliance.compliance_score:.2f}")
        print(f"   ‚úÖ Violations: {len(validated_non_compliant.constitution_compliance.violations)}")
    else:
        print("   ‚ùå No compliance data available")

    # Test 4: Constitution Error Mapping
    print("\n4. Testing Constitution Error Mapping...")

    # Map constitution failures to errors
    constitution_errors = []

    if validated_non_compliant.constitution_compliance:
        for violation in validated_non_compliant.constitution_compliance.violations:
            if "validation" in violation.lower():
                error = ConstitutionErrorMapper.map_constitution_failure_to_error(
                    failure_mode="validation_failure",
                    message=f"Constitution validation failed: {violation}",
                    severity=ErrorSeverity.HIGH,
                )
                constitution_errors.append(error)

    print(f"   ‚úÖ Constitution errors mapped: {len(constitution_errors)} errors")

    # Test 5: Error Handling Metrics
    print("\n5. Testing Error Handling Metrics...")

    all_errors = errors + constitution_errors
    metrics = ErrorClassifier.get_error_handling_metrics(all_errors)

    print(f"   ‚úÖ Total errors: {metrics['total_errors']}")
    print(f"   ‚úÖ Error types: {metrics['error_types']}")
    print(f"   ‚úÖ Severity distribution: {metrics['severity_distribution']}")
    print(f"   ‚úÖ Average severity score: {metrics['avg_severity_score']:.2f}")
    print(f"   ‚úÖ Most common error type: {metrics['most_common_error_type']}")
    print(f"   ‚úÖ Critical errors: {metrics['critical_errors_count']}")

    # Test 6: Performance Validation
    print("\n6. Testing Performance...")

    start_time = time.time()

    # Create multiple contexts
    for i in range(100):
        ContextFactory.create_context(
            AIRole.PLANNER,
            session_id=f"perf-test-{i}",
            project_scope="Performance test project scope",
            backlog_priority="P1",
        )

    context_creation_time = time.time() - start_time

    start_time = time.time()

    # Validate multiple outputs
    for i in range(50):
        output = ProgramOutput(
            output_content=f"Performance test output {i} with sufficient content length",
            output_type="text",
            context=planner_context,
            constitution_compliance=None,
        )
        aware_validator.validate_program_output(output)

    validation_time = time.time() - start_time

    print(f"   ‚úÖ Context creation: {context_creation_time:.3f}s for 100 contexts")
    print(f"   ‚úÖ Output validation: {validation_time:.3f}s for 50 outputs")
    print(f"   ‚úÖ Average context creation: {(context_creation_time/100)*1000:.2f}ms")
    print(f"   ‚úÖ Average validation: {(validation_time/50)*1000:.2f}ms")

    # Validate performance requirements
    avg_context_time = (context_creation_time / 100) * 1000
    avg_validation_time = (validation_time / 50) * 1000

    assert avg_context_time < 1.0, f"Context creation too slow: {avg_context_time:.2f}ms"
    assert avg_validation_time < 10.0, f"Validation too slow: {avg_validation_time:.2f}ms"

    print("   ‚úÖ Performance requirements met!")

    # Test 7: Backward Compatibility
    print("\n7. Testing Backward Compatibility...")

    from src.dspy_modules.context_models import LegacyContextAdapter

    # Test legacy adapter
    legacy_data = {
        "role": "planner",
        "session_id": "legacy-test",
        "project_scope": "Legacy compatibility test",
        "backlog_priority": "P1",
    }

    legacy_context = LegacyContextAdapter.from_dict(legacy_data)
    legacy_dict = LegacyContextAdapter.to_dict(legacy_context)

    print(f"   ‚úÖ Legacy adapter from_dict: {legacy_context.role.value}")
    print(f"   ‚úÖ Legacy adapter to_dict: {legacy_dict['role']}")

    assert legacy_context.role.value == "planner"
    assert legacy_dict["role"] == "planner"

    print("   ‚úÖ Backward compatibility working!")

    print("\n" + "=" * 70)
    print("üéâ B-1007 Integration Test PASSED!")
    print("‚úÖ All components working together successfully")
    print("‚úÖ Performance requirements met")
    print("‚úÖ Backward compatibility maintained")
    print("‚úÖ Constitution-aware validation operational")
    print("‚úÖ Error taxonomy providing structured error handling")
    print("‚úÖ Role-based context models functional")

    return True


if __name__ == "__main__":
    test_b1007_integration()
