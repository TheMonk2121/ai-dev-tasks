
# **Updated Schema Instructions (Version 21.0)**

## ðŸ“Œ **Overview**
This updated schema framework enhances **data lineage tracking, schema validation, processing scalability, and structured BI output compatibility**. These updates ensure **seamless integration between structured datasets, fact/dimension tables, and analytical workflows.**

---

## **ðŸ”¹ Key Enhancements in Version 21.0**
âœ… **Multi-Line Header Merging with Variable-Length Row Handling**  
âœ… **Pre-Fuzzy Alias Mapping to Improve Performance & Accuracy**  
âœ… **Expanded Logging (Borderline Matches & Summarized Schema Violations)**  
âœ… **Centralized & Incremental Logging for Easier Querying**  
âœ… **Optimized Referential Integrity & Type Enforcement**  
âœ… **Manual Override for File-Specific Aliases Before Global Dictionary**  
âœ… **Expanded Synthetic Test Coverage for CI/CD Pipelines**  

---

## **ðŸ”¹ Updated Schema Workflow**  

```
ðŸ“‚ Raw Data (CSV, Excel, JSON, API)  
   â¬‡  
ðŸ›  Data Validation & Schema Processing  
   â¬‡  
ðŸ“Š Fact & Dimension Table Structuring  
   â¬‡  
ðŸ“‘ Structured Output for BI & Analytics  
```  

---

## **ðŸ”¹ Multi-Line Header Merging & Variable-Length Row Handling**  

âœ… **Detects & fixes `NaT` headers by merging multiple rows dynamically**  
âœ… **Handles cases where row counts differ across header rows**  
âœ… **Filters out blank header components before merging**  

```python
def merge_multi_row_headers(df, header_rows):
    """Merges multiple header rows into a single header row, handling missing values."""
    max_cols = max(len(df.iloc[row].dropna()) for row in header_rows)  # Get max column count

    merged_headers = []
    for col_idx in range(max_cols):
        merged_parts = []
        for row in header_rows:
            cell_value = str(df.iloc[row, col_idx]).strip() if col_idx < len(df.iloc[row]) else ""
            if cell_value:
                merged_parts.append(cell_value)
        merged_headers.append("_".join(merged_parts) if merged_parts else f"ExtraCol_{col_idx}")

    return merged_headers
```  

---

## **ðŸ”¹ Alias Dictionary Applied Before Fuzzy Matching**  

âœ… **Ensures file-specific alias overrides are applied before the global alias dictionary**  
âœ… **Reduces reliance on fuzzy matching by handling known synonyms first**  

```python
ALIAS_DICT = {
  "cpi_val": "cpi_value",
  "cpi value ": "cpi_value",
  "expenditure_cat": "category"
}

def apply_aliases(col_name, file_name=None):
    """Maps column names using file-specific overrides first, then applies global alias dictionary."""
    if file_name in FILE_SPECIFIC_ALIASES and col_name in FILE_SPECIFIC_ALIASES[file_name]:
        return FILE_SPECIFIC_ALIASES[file_name][col_name]
    return ALIAS_DICT.get(col_name, col_name)
```  

---

## **ðŸ”¹ Logging Schema Validation Issues with Summarized Results**  

âœ… **Logs missing and unexpected columns in a structured format**  
âœ… **Summarizes missing foreign keys instead of logging every row-level issue**  
âœ… **Supports file-specific schema violation tracking**  

```python
def log_schema_violations(file_name, missing_keys, schema_warnings):
    """Logs missing and unexpected schema violations for better tracking."""
    log_data = {
        "file": file_name,
        "missing_keys_count": len(missing_keys),
        "warnings": schema_warnings
    }
    with open("schema_violations.json", "a") as f:
        json.dump(log_data, f, indent=4)
```  

---

## **ðŸ”¹ Referential Integrity & Data Type Enforcement Enhancements**  

âœ… **Prevents invalid foreign keys from entering structured datasets**  
âœ… **Logs only a count of missing foreign keys instead of row-level details**  

```python
def check_referential_integrity(fact_df, dim_df, key_column):
    """Ensures all fact table keys exist in the dimension table and logs summary results."""
    missing_keys = set(fact_df[key_column]) - set(dim_df[key_column])
    return {"missing_keys_count": len(missing_keys), "status": "All keys validated" if not missing_keys else "Integrity check failed"}
```  

---

## **ðŸ”¹ Expanded Test Coverage with Synthetic Data for CI/CD Pipelines**  

âœ… **Validates against a library of known problem cases before production release**  
âœ… **Ensures every update fixes prior issues without introducing new regressions**  

âœ” **Test Cases Covered:**  
  - **Multi-line headers with missing values**  
  - **Files with disclaimers and irrelevant rows**  
  - **Files with alias mismatches & fuzzy borderline cases**  
  - **Files with missing foreign keys & type mismatches**  

âœ” **Benefit:** **Prevents regressions and ensures consistent improvements across versions.**  

---

## **ðŸ”¹ Summary of Version 21.0 Updates**  
âœ… **Merges multi-line headers dynamically while handling blank cells and different row lengths**  
âœ… **Ensures alias dictionary is applied before fuzzy matching to improve performance**  
âœ… **Logs schema validation results in a structured format with summary reports**  
âœ… **Optimizes referential integrity checks to prevent excessive logging of missing keys**  
âœ… **Expands synthetic test coverage to validate edge cases in CI/CD pipelines**  

This ensures **greater resilience when dealing with real-world messy datasets.** ðŸš€

